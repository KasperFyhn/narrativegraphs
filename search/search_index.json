{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"narrativegraphs","text":"<p>Turn a collection of texts into an interactive narrative graph of entities and their relations and explore the structure of your corpus visually.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install narrativegraphs\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from narrativegraphs import NarrativeGraph\n\ndocs: list[str] = [...]  # your list of documents\nmodel = NarrativeGraph().fit(docs)\nmodel.serve_visualizer()\n</code></pre> <p>Open the link in your terminal to explore the graph in your browser:</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Plug'n'play solution \u2013 get started with a few lines of code</li> <li>Interactive browser-based visualizer \u2013 shipped with an interactive React app which can be hosted directly from Python, no extra dependencies</li> <li>See the original contexts that extracted entities and relations appear in</li> <li>Filter and query the graph by statistics, category, or timestamps</li> <li>Export graph and data to NetworkX and Pandas for your own custom analyses</li> <li>Modular structure \u2013 customize or switch out pipeline components to accommodate your use case.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation and tutorials: kasperfyhn.github.io/narrativegraphs</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this package in academic work, please cite:</p> <pre><code>@software{narrativegraphs,\n  author = {Fyhn, Kasper},\n  title = {narrativegraphs: A Python package for narrative graph analysis},\n  year = {2026},\n  url = {https://github.com/kasperfyhn/narrativegraphs}\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! This document explains how to set up a development environment and contribute to <code>narrativegraphs</code>.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/kasperilarsen/narrativegraphs.git\ncd narrativegraphs\n</code></pre> </li> <li> <p>Create a virtual environment and install in editable mode:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e \".[dev]\"\n</code></pre> </li> <li> <p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>This project uses:</p> <ul> <li><code>ruff</code> for linting and formatting Python code</li> <li>Type hints throughout (checked with <code>mypy</code>)</li> <li>Google-style docstrings</li> <li><code>eslint</code> for linting React/TypeScript code and <code>prettier</code> for formatting </li> <li></li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/your-feature</code></li> <li>Make your changes with tests</li> <li>Run the test suite and linters</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub or reach out directly.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all public modules, classes, and functions in <code>narrativegraphs</code>.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>narrativegraphs/\n\u251c\u2500\u2500 narrativegraph.py        # Core graph classes\n</code></pre>"},{"location":"api/#quick-links","title":"Quick Links","text":"<ul> <li><code>NarrativeGraph</code> \u2014 Main class for creating and querying graphs</li> </ul>"},{"location":"api/graph/","title":"Graph Module","text":"<p>The core module containing the <code>NarrativeGraph</code> class and related data structures.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph","title":"<code>NarrativeGraph</code>","text":"<p>               Bases: <code>BaseGraph</code></p> <p>Full narrative graph with triplet extraction, relations, and co-occurrences.</p> <p>NarrativeGraph extracts subject-predicate-object triplets from text documents and builds both a directed relation graph and an undirected co-occurrence graph.</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>class NarrativeGraph(BaseGraph):\n    \"\"\"Full narrative graph with triplet extraction, relations, and co-occurrences.\n\n    NarrativeGraph extracts subject-predicate-object triplets from text documents\n    and builds both a directed relation graph and an undirected co-occurrence graph.\n    \"\"\"\n\n    _cooccurrence_only = False\n\n    def __init__(\n        self,\n        triplet_extractor: TripletExtractor = None,\n        cooccurrence_extractor: CooccurrenceExtractor = None,\n        entity_mapper: Mapper = None,\n        predicate_mapper: Mapper = None,\n        sqlite_db_path: str = None,\n        on_existing_db: Literal[\"stop\", \"overwrite\", \"reuse\"] = \"stop\",\n        n_cpu: int = -1,\n    ):\n        \"\"\"Initialize a NarrativeGraph.\n\n        Args:\n            triplet_extractor: Extractor for subject-predicate-object triplets.\n            cooccurrence_extractor: Extractor for entity co-occurrences.\n            entity_mapper: Mapper for entity normalization.\n            predicate_mapper: Mapper for predicate normalization.\n            sqlite_db_path: Path to SQLite database file. If None, uses in-memory DB.\n            on_existing_db: Behavior when database exists:\n                - \"stop\": Raise error if DB contains data\n                - \"overwrite\": Delete existing DB\n                - \"reuse\": Use existing DB data\n            n_cpu: Number of CPUs for parallel processing (-1 for all).\n        \"\"\"\n        super().__init__(sqlite_db_path, on_existing_db)\n        self._pipeline = Pipeline(\n            self._engine,\n            triplet_extractor=triplet_extractor,\n            cooccurrence_extractor=cooccurrence_extractor,\n            entity_mapper=entity_mapper,\n            predicate_mapper=predicate_mapper,\n            n_cpu=n_cpu,\n        )\n\n    def fit(\n        self,\n        docs: list[str],\n        doc_ids: list[int | str] = None,\n        timestamps: list[datetime | date] = None,\n        categories: (\n            list[str | list[str]]\n            | dict[str, list[str | list[str]]]\n            | list[dict[str, str | list[str]]]\n        ) = None,\n    ) -&gt; \"NarrativeGraph\":\n        \"\"\"\n        Fit a narrative graph from documents. The docs can be accompanied by lists with\n        the same length of IDs, timestamps and categories.\n\n        Args:\n            docs: Required argument, a list of documents as strings.\n            doc_ids: Optional list of document ids. Same length as docs.\n            timestamps: Optional list of document timestamps. Same length as docs.\n            categories: Optional list of document categories. Supports single or\n                multiple categories. A document can have a single or multiple labels\n                per category. See further down for examples.\n\n        Returns:\n            A fitted NarrativeGraph instance.\n\n        \"\"\"\n        self._pipeline.run(\n            docs,\n            doc_ids=doc_ids,\n            timestamps=timestamps,\n            categories=categories,\n        )\n        return self\n\n    @property\n    def predicates_(self) -&gt; pd.DataFrame:\n        \"\"\"Predicates as a pandas DataFrame.\"\"\"\n        return self.predicates.as_df()\n\n    @property\n    def relations_(self) -&gt; pd.DataFrame:\n        \"\"\"Relations as a pandas DataFrame.\"\"\"\n        return self.relations.as_df()\n\n    @property\n    def triplets_(self) -&gt; pd.DataFrame:\n        \"\"\"Triplets as a pandas DataFrame.\"\"\"\n        return self.triplets.as_df()\n\n    @property\n    def relation_graph_(self) -&gt; nx.DiGraph:\n        \"\"\"The full relation graph as a directed NetworkX graph.\"\"\"\n        rg = self.graph.get_graph(\"relation\")\n        g = nx.DiGraph()\n        g.add_nodes_from((n.id, n) for n in rg.nodes)\n        g.add_edges_from((e.from_id, e.to_id, e) for e in rg.edges)\n        return g\n\n    @classmethod\n    def load(cls, file_path: str) -&gt; \"NarrativeGraph\":\n        \"\"\"\n\n        Args:\n            file_path: path to a SQLite database to load a NarrativeGraph from.\n\n        Returns:\n            A NarrativeGraph object\n        \"\"\"\n        return super().load(file_path)  # noqa\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.predicates_","title":"<code>predicates_</code>  <code>property</code>","text":"<p>Predicates as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.relations_","title":"<code>relations_</code>  <code>property</code>","text":"<p>Relations as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.triplets_","title":"<code>triplets_</code>  <code>property</code>","text":"<p>Triplets as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.relation_graph_","title":"<code>relation_graph_</code>  <code>property</code>","text":"<p>The full relation graph as a directed NetworkX graph.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.__init__","title":"<code>__init__(triplet_extractor=None, cooccurrence_extractor=None, entity_mapper=None, predicate_mapper=None, sqlite_db_path=None, on_existing_db='stop', n_cpu=-1)</code>","text":"<p>Initialize a NarrativeGraph.</p> <p>Parameters:</p> Name Type Description Default <code>triplet_extractor</code> <code>TripletExtractor</code> <p>Extractor for subject-predicate-object triplets.</p> <code>None</code> <code>cooccurrence_extractor</code> <code>CooccurrenceExtractor</code> <p>Extractor for entity co-occurrences.</p> <code>None</code> <code>entity_mapper</code> <code>Mapper</code> <p>Mapper for entity normalization.</p> <code>None</code> <code>predicate_mapper</code> <code>Mapper</code> <p>Mapper for predicate normalization.</p> <code>None</code> <code>sqlite_db_path</code> <code>str</code> <p>Path to SQLite database file. If None, uses in-memory DB.</p> <code>None</code> <code>on_existing_db</code> <code>Literal['stop', 'overwrite', 'reuse']</code> <p>Behavior when database exists: - \"stop\": Raise error if DB contains data - \"overwrite\": Delete existing DB - \"reuse\": Use existing DB data</p> <code>'stop'</code> <code>n_cpu</code> <code>int</code> <p>Number of CPUs for parallel processing (-1 for all).</p> <code>-1</code> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>def __init__(\n    self,\n    triplet_extractor: TripletExtractor = None,\n    cooccurrence_extractor: CooccurrenceExtractor = None,\n    entity_mapper: Mapper = None,\n    predicate_mapper: Mapper = None,\n    sqlite_db_path: str = None,\n    on_existing_db: Literal[\"stop\", \"overwrite\", \"reuse\"] = \"stop\",\n    n_cpu: int = -1,\n):\n    \"\"\"Initialize a NarrativeGraph.\n\n    Args:\n        triplet_extractor: Extractor for subject-predicate-object triplets.\n        cooccurrence_extractor: Extractor for entity co-occurrences.\n        entity_mapper: Mapper for entity normalization.\n        predicate_mapper: Mapper for predicate normalization.\n        sqlite_db_path: Path to SQLite database file. If None, uses in-memory DB.\n        on_existing_db: Behavior when database exists:\n            - \"stop\": Raise error if DB contains data\n            - \"overwrite\": Delete existing DB\n            - \"reuse\": Use existing DB data\n        n_cpu: Number of CPUs for parallel processing (-1 for all).\n    \"\"\"\n    super().__init__(sqlite_db_path, on_existing_db)\n    self._pipeline = Pipeline(\n        self._engine,\n        triplet_extractor=triplet_extractor,\n        cooccurrence_extractor=cooccurrence_extractor,\n        entity_mapper=entity_mapper,\n        predicate_mapper=predicate_mapper,\n        n_cpu=n_cpu,\n    )\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.fit","title":"<code>fit(docs, doc_ids=None, timestamps=None, categories=None)</code>","text":"<p>Fit a narrative graph from documents. The docs can be accompanied by lists with the same length of IDs, timestamps and categories.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[str]</code> <p>Required argument, a list of documents as strings.</p> required <code>doc_ids</code> <code>list[int | str]</code> <p>Optional list of document ids. Same length as docs.</p> <code>None</code> <code>timestamps</code> <code>list[datetime | date]</code> <p>Optional list of document timestamps. Same length as docs.</p> <code>None</code> <code>categories</code> <code>list[str | list[str]] | dict[str, list[str | list[str]]] | list[dict[str, str | list[str]]]</code> <p>Optional list of document categories. Supports single or multiple categories. A document can have a single or multiple labels per category. See further down for examples.</p> <code>None</code> <p>Returns:</p> Type Description <code>NarrativeGraph</code> <p>A fitted NarrativeGraph instance.</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>def fit(\n    self,\n    docs: list[str],\n    doc_ids: list[int | str] = None,\n    timestamps: list[datetime | date] = None,\n    categories: (\n        list[str | list[str]]\n        | dict[str, list[str | list[str]]]\n        | list[dict[str, str | list[str]]]\n    ) = None,\n) -&gt; \"NarrativeGraph\":\n    \"\"\"\n    Fit a narrative graph from documents. The docs can be accompanied by lists with\n    the same length of IDs, timestamps and categories.\n\n    Args:\n        docs: Required argument, a list of documents as strings.\n        doc_ids: Optional list of document ids. Same length as docs.\n        timestamps: Optional list of document timestamps. Same length as docs.\n        categories: Optional list of document categories. Supports single or\n            multiple categories. A document can have a single or multiple labels\n            per category. See further down for examples.\n\n    Returns:\n        A fitted NarrativeGraph instance.\n\n    \"\"\"\n    self._pipeline.run(\n        docs,\n        doc_ids=doc_ids,\n        timestamps=timestamps,\n        categories=categories,\n    )\n    return self\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a SQLite database to load a NarrativeGraph from.</p> required <p>Returns:</p> Type Description <code>NarrativeGraph</code> <p>A NarrativeGraph object</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>@classmethod\ndef load(cls, file_path: str) -&gt; \"NarrativeGraph\":\n    \"\"\"\n\n    Args:\n        file_path: path to a SQLite database to load a NarrativeGraph from.\n\n    Returns:\n        A NarrativeGraph object\n    \"\"\"\n    return super().load(file_path)  # noqa\n</code></pre>"},{"location":"api/triplet_extraction/","title":"Extraction Module","text":""},{"location":"api/triplet_extraction/#tripletextractor","title":"TripletExtractor","text":""},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor","title":"<code>TripletExtractor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for triplet extraction algorithms.</p> <p>Triplets are instantiated as Triplet objects that consist of SpanAnnotation objects.</p> <p>Thus, to create a Triplet, you create the</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>class TripletExtractor(ABC):\n    \"\"\"\n    Abstract base class for triplet extraction algorithms.\n\n    Triplets are instantiated as Triplet objects that consist of SpanAnnotation objects.\n\n    Thus, to create a Triplet, you create the\n    \"\"\"\n\n    @abstractmethod\n    def extract(self, text: str) -&gt; list[Triplet]:\n        \"\"\"Single document extraction\n        Args:\n            text: a raw text string\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        pass\n\n    def batch_extract(\n        self, texts: Iterable[str], n_cpu: int = 1, **kwargs\n    ) -&gt; Generator[list[Triplet], None, None]:\n        \"\"\"Multiple-document extraction\n        Args:\n            texts: an iterable of raw text strings; may be a generator, so be mindful\n                of consuming items\n            n_cpu: number of CPUs to use\n            **kwargs: other keyword arguments for your own class\n\n        Returns:\n            should yield triplets per text in the same order as texts iterable\n\n        \"\"\"\n        for text in texts:\n            yield self.extract(text)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor.extract","title":"<code>extract(text)</code>  <code>abstractmethod</code>","text":"<p>Single document extraction Args:     text: a raw text string</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>@abstractmethod\ndef extract(self, text: str) -&gt; list[Triplet]:\n    \"\"\"Single document extraction\n    Args:\n        text: a raw text string\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor.batch_extract","title":"<code>batch_extract(texts, n_cpu=1, **kwargs)</code>","text":"<p>Multiple-document extraction Args:     texts: an iterable of raw text strings; may be a generator, so be mindful         of consuming items     n_cpu: number of CPUs to use     **kwargs: other keyword arguments for your own class</p> <p>Returns:</p> Type Description <code>None</code> <p>should yield triplets per text in the same order as texts iterable</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>def batch_extract(\n    self, texts: Iterable[str], n_cpu: int = 1, **kwargs\n) -&gt; Generator[list[Triplet], None, None]:\n    \"\"\"Multiple-document extraction\n    Args:\n        texts: an iterable of raw text strings; may be a generator, so be mindful\n            of consuming items\n        n_cpu: number of CPUs to use\n        **kwargs: other keyword arguments for your own class\n\n    Returns:\n        should yield triplets per text in the same order as texts iterable\n\n    \"\"\"\n    for text in texts:\n        yield self.extract(text)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor","title":"<code>SpacyTripletExtractor</code>","text":"<p>               Bases: <code>TripletExtractor</code></p> <p>Base class for implementing triplet extraction based on spaCy docs.</p> <p>Override <code>extract_triplets_from_sent</code> for extracting triplets sentence by sentence.</p> <p>Override <code>extract_triplets_from_doc</code> for extracting with the full Doc context.</p> <p>The <code>SpanAnnotation</code> objects of <code>Triplet</code> objects can conveniently be created from a spaCy <code>Span</code> object with <code>SpanAnnotation.from_span()</code>.</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>class SpacyTripletExtractor(TripletExtractor):\n    \"\"\"Base class for implementing triplet extraction based on spaCy docs.\n\n    Override `extract_triplets_from_sent` for extracting triplets sentence by sentence.\n\n    Override `extract_triplets_from_doc` for extracting with the full Doc context.\n\n    The `SpanAnnotation` objects of `Triplet` objects can conveniently be created from\n    a spaCy `Span` object with `SpanAnnotation.from_span()`.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str = None, split_sentence_on_double_line_break: bool = True\n    ):\n        \"\"\"\n        Args:\n            model_name: name of the spaCy model to use\n            split_sentence_on_double_line_break: adds extra sentence boundaries on\n                double line breaks (\"\\n\\n\")\n        \"\"\"\n        if model_name is None:\n            model_name = \"en_core_web_sm\"\n        self.nlp = ensure_spacy_model(model_name)\n        if split_sentence_on_double_line_break:\n            self.nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")\n\n    @abstractmethod\n    def extract_triplets_from_sent(self, sent: Span) -&gt; list[Triplet]:\n        \"\"\"Extract triplets from a SpaCy sentence.\n        Args:\n            sent: A SpaCy Span object representing the whole sentence\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        pass\n\n    def extract_triplets_from_doc(self, doc: Doc) -&gt; list[Triplet]:\n        \"\"\"Extract triplets from a Doc\n        Args:\n            doc: A SpaCy Doc object\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        triplets = []\n        for sent in doc.sents:\n            sent_triplets = self.extract_triplets_from_sent(sent)\n            if sent_triplets is not None:\n                triplets.extend(sent_triplets)\n        return triplets\n\n    def extract(self, text: str) -&gt; list[Triplet]:\n        text = self.nlp(text)\n        return self.extract_triplets_from_doc(text)\n\n    def batch_extract(\n        self, texts: list[str], n_cpu: int = 1, batch_size: int = None\n    ) -&gt; Generator[list[Triplet], None, None]:\n        if batch_size is None:\n            batch_size = calculate_batch_size(texts, n_cpu)\n        _logger.info(\"Using multiple CPU cores.Progress bars may stand still at first.\")\n        for doc in self.nlp.pipe(texts, n_process=n_cpu, batch_size=batch_size):\n            yield self.extract_triplets_from_doc(doc)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.__init__","title":"<code>__init__(model_name=None, split_sentence_on_double_line_break=True)</code>","text":"<pre><code>    Args:\n        model_name: name of the spaCy model to use\n        split_sentence_on_double_line_break: adds extra sentence boundaries on\n            double line breaks (\"\n</code></pre> <p>\")</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>def __init__(\n    self, model_name: str = None, split_sentence_on_double_line_break: bool = True\n):\n    \"\"\"\n    Args:\n        model_name: name of the spaCy model to use\n        split_sentence_on_double_line_break: adds extra sentence boundaries on\n            double line breaks (\"\\n\\n\")\n    \"\"\"\n    if model_name is None:\n        model_name = \"en_core_web_sm\"\n    self.nlp = ensure_spacy_model(model_name)\n    if split_sentence_on_double_line_break:\n        self.nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.extract_triplets_from_sent","title":"<code>extract_triplets_from_sent(sent)</code>  <code>abstractmethod</code>","text":"<p>Extract triplets from a SpaCy sentence. Args:     sent: A SpaCy Span object representing the whole sentence</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>@abstractmethod\ndef extract_triplets_from_sent(self, sent: Span) -&gt; list[Triplet]:\n    \"\"\"Extract triplets from a SpaCy sentence.\n    Args:\n        sent: A SpaCy Span object representing the whole sentence\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.extract_triplets_from_doc","title":"<code>extract_triplets_from_doc(doc)</code>","text":"<p>Extract triplets from a Doc Args:     doc: A SpaCy Doc object</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>def extract_triplets_from_doc(self, doc: Doc) -&gt; list[Triplet]:\n    \"\"\"Extract triplets from a Doc\n    Args:\n        doc: A SpaCy Doc object\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    triplets = []\n    for sent in doc.sents:\n        sent_triplets = self.extract_triplets_from_sent(sent)\n        if sent_triplets is not None:\n            triplets.extend(sent_triplets)\n    return triplets\n</code></pre>"},{"location":"getting-started/adding-metadata/","title":"Metadata","text":"<p>The <code>NarrativeGraph.fit()</code> method takes a few extra optional parameters that serve as metadata for your docs. These are document IDs, timestamps and categories.</p> <p>If available for your data, it can be valuable. Timestamps and categories will give some more options for slicing the graph data in the visualizer. IDs are mostly a reference point if you are looking for a specific document.</p> <p>They should be served as lists of the same length.</p> <pre><code>from narrativegraphs import NarrativeGraph\nfrom datetime import date\n\ndocs: list[str] = [...]  # your list of documents\ndoc_ids: list[int] = [...]  # your list of document IDs\ntimestamps: list[date] = [...]  # your list of dates\ncategories: list[str] = [...]  # your list of categories as a string\nmodel = NarrativeGraph().fit(\n    docs,\n    doc_ids=doc_ids,\n    timestamps=timestamps,\n    categories=categories\n)\nmodel.serve_visualizer()\n</code></pre>"},{"location":"getting-started/create-and-inspect/","title":"Create and inspect","text":"<p>The basic workflow of creating and inspecting a narrative graph is:</p> <ol> <li>Import <code>narrativegraphs</code>.</li> <li>Prepare your documents as a list of strings.</li> <li>Initialize a model and fit it on your docs.</li> <li>Serve the visualizer, follow the link, and inspect your docs visually.</li> </ol> <pre><code>from narrativegraphs import NarrativeGraph\n\ndocs: list[str] = [...]  # your list of documents\nmodel = NarrativeGraph().fit(docs)\nmodel.serve_visualizer()\n</code></pre> <p>Open the link in your terminal to explore the graph in your browser:</p> <p></p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install narrativegraphs\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>For the latest development version:</p> <pre><code>pip install git+https://github.com/kasperfyhn/narrativegraphs.git\n</code></pre> <p>Or clone and install in editable mode for development:</p> <pre><code>git clone https://github.com/kasperfyhn/narrativegraphs.git\ncd narrativegraphs\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#setting-up-spacy-models","title":"Setting Up spaCy Models","text":"<p>After installation, download the spaCy model(s) you need:</p> <pre><code>python -m spacy download en_core_web_sm  # English, small\npython -m spacy download da_core_news_sm  # Danish, small\n</code></pre>"},{"location":"getting-started/save-and-load/","title":"Saving and loading","text":"<p>A model can be saved and loaded for later use, so you do not have to re-process documents every time.</p> <p>To save:</p> <pre><code>model.save_to_file(\"my_model.db\")\n</code></pre> <p>To load it again</p> <pre><code>NarrativeGraph.load(\"my_model.db\")\n</code></pre> <p>A handy codeblock is: <pre><code>from narrativegraphs import NarrativeGraph\nimport os.path\n\nmodel_name = \"my_model.db\"\ndocs: list[str] = [...]  # your list of documents\n\nif os.path.exists(model_name):\n    print(\"Loading model!\")\n    model = NarrativeGraph.load(model_name)\nelse:\n    print(\"Creating and saving model!\")\n    model = NarrativeGraph().fit(docs)\n    model.save_to_file(model_name)\n\nmodel.serve_visualizer()\n</code></pre></p>"},{"location":"notebooks/categories/","title":"Categories metadata","text":"In\u00a0[1]: Copied! <pre>single_level_single_label = [\"Politics\", \"Sports\", \"Food\"]\n</pre> single_level_single_label = [\"Politics\", \"Sports\", \"Food\"] In\u00a0[2]: Copied! <pre>single_level_multi_label = [\n    [\"Politics\", \"Celebrities\"],\n    [\"Sports\", \"Celebrities\"],\n    [\"Sports\", \"Food\"]\n]\n</pre> single_level_multi_label = [     [\"Politics\", \"Celebrities\"],     [\"Sports\", \"Celebrities\"],     [\"Sports\", \"Food\"] ] In\u00a0[\u00a0]: Copied! <pre>multi_level_multi_label = {\n    \"section\": [\n        [\"Politics\", \"Celebrities\"],\n        [\"Sports\", \"Celebrities\"],\n        [\"Sports\", \"Food\"]\n    ],\n    \"sentiment\": [\"positive\", \"negative\", \"positive\"]\n}\n\nmulti_level_multi_label = [\n    {\"section\": [\"Politics\", \"Celebrities\"], \"sentiment\": \"positive\"},\n    {\"section\": [\"Sports\", \"Celebrities\"], \"sentiment\": \"negative\"},\n    {\"section\": [\"Sports\", \"Food\"], \"sentiment\": \"positive\"}\n]\n</pre> multi_level_multi_label = {     \"section\": [         [\"Politics\", \"Celebrities\"],         [\"Sports\", \"Celebrities\"],         [\"Sports\", \"Food\"]     ],     \"sentiment\": [\"positive\", \"negative\", \"positive\"] }  multi_level_multi_label = [     {\"section\": [\"Politics\", \"Celebrities\"], \"sentiment\": \"positive\"},     {\"section\": [\"Sports\", \"Celebrities\"], \"sentiment\": \"negative\"},     {\"section\": [\"Sports\", \"Food\"], \"sentiment\": \"positive\"} ]"},{"location":"notebooks/categories/#categories-metadata","title":"Categories metadata\u00b6","text":"<p>When you fit a <code>NarrativeGraph</code> object, the <code>categories</code> argument can be given in many different ways. This is because the system supports various levels of categorization.</p>"},{"location":"notebooks/categories/#single-level-single-labels","title":"Single level, single labels\u00b6","text":"<p>The simplest case is when you have on type of category. That could be the newspaper section that a document is from. This is given as a single list with a single label per category.</p>"},{"location":"notebooks/categories/#single-level-multiple-labels","title":"Single level, multiple labels\u00b6","text":"<p>A similar case is when you have one type of category, but where a single document can have multiple labels for that category. It could be newspaper thematic/topical tags. This is given as a single list with a list of labels for each document.</p>"},{"location":"notebooks/categories/#multiple-levels-variable-levels","title":"Multiple levels, variable levels\u00b6","text":"<p>The more complex case is when you have multiple categories, and each of these categories may behave differently. It could be the tags from above in one category (multi-label) and sentiment in another (single-label).</p> <p>This can be given in two ways: a dict with list values or a list with dict entries.</p>"},{"location":"notebooks/demo/","title":"Demo: Creating and inspecting a narrative graph","text":"<p>This notebook will serve as a demo and small tour of some of the core functionalities of a <code>NarrativeGraph</code> object.</p> In\u00a0[1]: Copied! <pre>from kagglehub import KaggleDatasetAdapter\nimport kagglehub\n\ndata = kagglehub.dataset_load(\n    KaggleDatasetAdapter.PANDAS,\n    \"rmisra/news-category-dataset\",\n    \"News_Category_Dataset_v3.json\",\n    pandas_kwargs=dict(lines=True),\n)\ndata.head()\n</pre> from kagglehub import KaggleDatasetAdapter import kagglehub  data = kagglehub.dataset_load(     KaggleDatasetAdapter.PANDAS,     \"rmisra/news-category-dataset\",     \"News_Category_Dataset_v3.json\",     pandas_kwargs=dict(lines=True), ) data.head() Out[1]: link headline category short_description authors date 0 https://www.huffpost.com/entry/covid-boosters-... Over 4 Million Americans Roll Up Sleeves For O... U.S. NEWS Health experts said it is too early to predict... Carla K. Johnson, AP 2022-09-23 1 https://www.huffpost.com/entry/american-airlin... American Airlines Flyer Charged, Banned For Li... U.S. NEWS He was subdued by passengers and crew when he ... Mary Papenfuss 2022-09-23 2 https://www.huffpost.com/entry/funniest-tweets... 23 Of The Funniest Tweets About Cats And Dogs ... COMEDY \"Until you have a dog you don't understand wha... Elyse Wanshel 2022-09-23 3 https://www.huffpost.com/entry/funniest-parent... The Funniest Tweets From Parents This Week (Se... PARENTING \"Accidentally put grown-up toothpaste on my to... Caroline Bologna 2022-09-23 4 https://www.huffpost.com/entry/amy-cooper-lose... Woman Who Called Cops On Black Bird-Watcher Lo... U.S. NEWS Amy Cooper accused investment firm Franklin Te... Nina Golgowski 2022-09-22 <p>The columns that we will be using as input for our narrative graph.</p> <ul> <li>Documents: headline + short_description</li> <li>IDs: link, but without the part that is in all of them</li> <li>Timestamps: date</li> <li>Categories: category</li> </ul> <p>There are many categories. We will create a subset with just two of them: U.S. News and Politics.</p> In\u00a0[2]: Copied! <pre># create a sample\nsample = data[data[\"category\"].isin([\"U.S. NEWS\", \"POLITICS\"])].sample(\n    5000, random_state=42\n)\ndocs = sample[\"headline\"] + \"\\n\\n\" + sample[\"short_description\"]\nids = sample[\"link\"].replace(\"https://www.huffpost.com/entry/\", \"\")  # get rit of the first part of the URL\ncategories = sample[\"category\"]\ntimestamps = sample[\"date\"]\n</pre> # create a sample sample = data[data[\"category\"].isin([\"U.S. NEWS\", \"POLITICS\"])].sample(     5000, random_state=42 ) docs = sample[\"headline\"] + \"\\n\\n\" + sample[\"short_description\"] ids = sample[\"link\"].replace(\"https://www.huffpost.com/entry/\", \"\")  # get rit of the first part of the URL categories = sample[\"category\"] timestamps = sample[\"date\"] In\u00a0[3]: Copied! <pre>from narrativegraphs import NarrativeGraph\n\nmodel = NarrativeGraph()\nmodel.fit(docs, doc_ids=ids, categories=categories, timestamps=timestamps)\n</pre> from narrativegraphs import NarrativeGraph  model = NarrativeGraph() model.fit(docs, doc_ids=ids, categories=categories, timestamps=timestamps) <pre>INFO:narrativegraphs.pipeline:Adding 5000 documents to database\nINFO:narrativegraphs.pipeline:Extracting triplets\nExtracting triplets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:43&lt;00:00, 113.66it/s] \nINFO:narrativegraphs.pipeline:Resolving entities and predicates\nINFO:narrativegraphs.pipeline:Mapping triplets and tuplets\nINFO:narrativegraphs.pipeline:Calculating stats\n</pre> Out[3]: <pre>&lt;narrativegraphs.narrativegraph.NarrativeGraph at 0x148052cc0&gt;</pre> In\u00a0[4]: Copied! <pre># create server to be viewed in own browser which blocks execution of other cells\nmodel.serve_visualizer()\n</pre> # create server to be viewed in own browser which blocks execution of other cells model.serve_visualizer() <pre>INFO:     Started server process [57523]\nINFO:     Waiting for application startup.\nINFO:root:Database engine provided to state before startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [57523]\nINFO:root:Server stopped by user\n</pre> <p>Stop it by hitting the stop button on the cell in Jupyter Notebook or hit CTRL+C elsewhere.</p> In\u00a0[13]: Copied! <pre>relation_graph = model.relation_graph_\n</pre> relation_graph = model.relation_graph_ In\u00a0[14]: Copied! <pre>print(type(relation_graph))\n</pre> print(type(relation_graph)) <pre>&lt;class 'networkx.classes.digraph.DiGraph'&gt;\n</pre> In\u00a0[15]: Copied! <pre>print(*list(relation_graph.nodes(data=True))[:3], sep=\"\\n\")\n</pre> print(*list(relation_graph.nodes(data=True))[:3], sep=\"\\n\") <pre>(1, {'id': 1, 'label': 'metal detectors', 'frequency': 3, 'focus': False})\n(2, {'id': 2, 'label': 'a seat', 'frequency': 4, 'focus': False})\n(3, {'id': 3, 'label': 'Jill Stein\u2019s', 'frequency': 1, 'focus': False})\n</pre> <p>Similarly, entities and relations and everything else can be accessed as <code>pandas.DataFrame</code>s through properties.</p> In\u00a0[16]: Copied! <pre>model.entities_\n</pre> model.entities_ Out[16]: id label frequency doc_frequency spread adjusted_tf_idf first_occurrence last_occurrence alt_labels category 0 1 metal detectors 3 2 0.0004 3333.333333 2020-02-02 2021-01-13 [\"metal detectors\"] [POLITICS, U.S. NEWS, POLITICS] 1 2 a seat 4 3 0.0006 3750.000000 2017-05-12 2018-02-28 [\"a seat\"] [POLITICS, POLITICS, POLITICS, POLITICS] 2 3 Jill Stein\u2019s 1 1 0.0002 0.000000 2016-12-02 2016-12-02 [\"Jill Stein\u2019s\"] [POLITICS] 3 4 The Senate's Stealth Raid 1 1 0.0002 0.000000 2017-06-14 2017-06-14 [\"The Senate's Stealth Raid\"] [POLITICS] 4 5 Trump-Style\\n\\n 1 1 0.0002 0.000000 2017-09-25 2017-09-25 [\"Trump-Style\\n\\n\"] [POLITICS] ... ... ... ... ... ... ... ... ... ... ... 12430 12431 Blames Obama 1 1 0.0002 0.000000 2018-10-24 2018-10-24 [\"Blames Obama\"] [POLITICS] 12431 12432 all the right moves 1 1 0.0002 0.000000 2016-02-14 2016-02-14 [\"all the right moves\"] [POLITICS] 12432 12433 the visitor's gallery 2 1 0.0002 2500.000000 2015-03-05 2015-03-05 [\"the visitor's gallery\"] [POLITICS, POLITICS] 12433 12434 the edge 1 1 0.0002 0.000000 2016-11-30 2016-11-30 [\"the edge\"] [POLITICS] 12434 12435 Food Stamps 2 1 0.0002 2500.000000 2016-06-21 2016-06-21 [\"Food Stamps\"] [POLITICS, POLITICS] <p>12435 rows \u00d7 10 columns</p> <p>The properties (with trailing <code>_</code>) are nice in that they give back the data in well-known formats that one can continue working with, e.g. NetworkX graphs for graph algorithms and DataFrames for statistical analyses.</p> In\u00a0[17]: Copied! <pre>white_house_matches = model.entities.search(\"White House\")\nwhite_house_matches[:10]\n</pre> white_house_matches = model.entities.search(\"White House\") white_house_matches[:10] Out[17]: <pre>[EntityLabel(id=617, label='White House'),\n EntityLabel(id=4597, label=\"White House's\"),\n EntityLabel(id=157, label='White House Story'),\n EntityLabel(id=389, label='White House Protest'),\n EntityLabel(id=6105, label='White House On Health Care\\n\\n'),\n EntityLabel(id=1251, label='Leave White House'),\n EntityLabel(id=6695, label='Obama White House'),\n EntityLabel(id=4229, label='Nominate White House'),\n EntityLabel(id=5900, label='The Trump White House'),\n EntityLabel(id=3020, label=\"the White House Correspondents' Dinner\")]</pre> In\u00a0[18]: Copied! <pre>white_house_id = white_house_matches[0].id\n</pre> white_house_id = white_house_matches[0].id <p>And you can create a subgraph that expands from a set of focus nodes and only includes those that pass a filter.</p> In\u00a0[19]: Copied! <pre>from datetime import date\nfrom narrativegraphs import GraphFilter\n\nwhite_house_graph = model.graph.expand_from_focus_entities(\n    [white_house_id],\n    \"relation\",\n    graph_filter=GraphFilter(\n        minimum_node_frequency=20,\n        categories={'category': [\"POLITICS\"]},\n        earliest_date=date(2014, 1, 1)\n    )\n)\n\n# stripping labels to remove some whitespaces\nprint(\"NODES\")\nfor node in white_house_graph.nodes:\n    print(node.id, node.label.strip())\n\nprint(\"\\nEDGES\")\nfor edge in white_house_graph.edges:\n    print(edge.subject_label.strip(), '-', edge.label, '-&gt;', edge.object_label.strip())\n</pre> from datetime import date from narrativegraphs import GraphFilter  white_house_graph = model.graph.expand_from_focus_entities(     [white_house_id],     \"relation\",     graph_filter=GraphFilter(         minimum_node_frequency=20,         categories={'category': [\"POLITICS\"]},         earliest_date=date(2014, 1, 1)     ) )  # stripping labels to remove some whitespaces print(\"NODES\") for node in white_house_graph.nodes:     print(node.id, node.label.strip())  print(\"\\nEDGES\") for edge in white_house_graph.edges:     print(edge.subject_label.strip(), '-', edge.label, '-&gt;', edge.object_label.strip()) <pre>NODES\n204 Ted Cruz\n391 DONALD Trump\n617 White House\n1600 Hillary Clinton\n2265 Rules\n2606 the bill\n3567 President Obama\n10348 last year\n\nEDGES\nTed Cruz - Rejected Bush -&gt; White House\nTed Cruz - ,, to take on -&gt; DONALD Trump\nDONALD Trump - and, Calls On, On, ... -&gt; Hillary Clinton\nDONALD Trump - Accuses, and, and Texas Sen., ... -&gt; Ted Cruz\nDONALD Trump - In -&gt; White House\nDONALD Trump - has rewritten -&gt; Rules\nWhite House - plans to scrap -&gt; Rules\nWhite House - after -&gt; President Obama\nHillary Clinton - Met For Lunch At, Win -&gt; White House\nHillary Clinton - and, Over, : -&gt; DONALD Trump\nthe bill - at -&gt; White House\nPresident Obama - and -&gt; Hillary Clinton\nlast year - at -&gt; White House\n</pre> <p>We can save the model for later use, especially if we have a lot of documents that takes a while to process.</p> In\u00a0[21]: Copied! <pre>model.save_to_file(\"demo\")\n</pre> model.save_to_file(\"demo\") <p>And we can load it from that saved file.</p> In\u00a0[22]: Copied! <pre>model = NarrativeGraph.load(\"demo\")\n</pre> model = NarrativeGraph.load(\"demo\")"},{"location":"notebooks/demo/#demo-creating-and-inspecting-a-narrative-graph","title":"Demo: Creating and inspecting a narrative graph\u00b6","text":""},{"location":"notebooks/demo/#data-setup","title":"Data setup\u00b6","text":"<p>For this demo notebook, we will be using News Category Dataset [1, 2] available on Kagglehub because it has short texts, timestamps and are categorized.</p>"},{"location":"notebooks/demo/#creating-the-model","title":"Creating the model\u00b6","text":"<p>Once we have our list of documents, which is the only required input, and extra metadata in aligned lists, we can create a narrative graph.</p>"},{"location":"notebooks/demo/#inspecting-the-model-visually","title":"Inspecting the model visually\u00b6","text":"<p>One of the key features of the narrativegraphs package is that it lets a user inspect the output interactively in a browser-based visualizer. It is hosted directly on your machine by the Python package \u2013 no extra dependencies required. This is achieved with the one line below.</p> <p>Click the link in the log messages to open in your browser.</p>"},{"location":"notebooks/demo/#inspecting-and-accessing-the-model-programmatically","title":"Inspecting and accessing the model programmatically\u00b6","text":"<p>The graph consists of entities as nodes and their relations or cooccurrences as edges. These, along with the data that back them, like documents and extracted semantic triplets, can be retrieved from the model through properties or service attributes.</p>"},{"location":"notebooks/demo/#attributes","title":"Attributes\u00b6","text":"<p>We can get the graph as a whole, as NetworkX graph, through the properties <code>.relation_graph_</code> and <code>.cooccurrence_graph_</code>.</p>"},{"location":"notebooks/demo/#service-attributes","title":"Service attributes\u00b6","text":"<p>However, the service attributes offer more control and may be especially handy if the model is quite big, so that you do not necessarily want everything spit out at once.</p> <p>For instance, you can search for entities with the <code>entities</code> service.</p>"},{"location":"notebooks/demo/#saving-and-loading-the-model","title":"Saving and loading the model\u00b6","text":""},{"location":"notebooks/demo/#references","title":"References\u00b6","text":"<p>[1] Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).</p> <p>[2] Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).</p>"}]}