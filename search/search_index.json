{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"narrativegraphs","text":"<p>Turn a collection of texts into an interactive narrative graph of entities and their relations and explore the structure of your corpus visually.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install narrativegraphs\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from narrativegraphs import NarrativeGraph\n\ndocs: list[str] = [...]  # your list of documents\nmodel = NarrativeGraph().fit(docs)\nmodel.serve_visualizer()\n</code></pre> <p>Open the link in your terminal to explore the graph in your browser:</p> <p></p>"},{"location":"#features","title":"Features","text":"<ul> <li>Plug'n'play solution \u2013 get started with a few lines of code</li> <li>Interactive browser-based visualizer \u2013 shipped with an interactive React app which can be hosted directly from Python, no extra dependencies</li> <li>See the original contexts that extracted entities and relations appear in</li> <li>Filter and query the graph by statistics, category, or timestamps</li> <li>Export graph and data to NetworkX and Pandas for your own custom analyses</li> <li>Modular structure \u2013 customize or switch out pipeline components to accommodate your use case.</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation and tutorials: kasperfyhn.github.io/narrativegraphs</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this package in academic work, please cite:</p> <pre><code>@software{narrativegraphs,\n  author = {Fyhn, Kasper},\n  title = {narrativegraphs: A Python package for narrative graph analysis},\n  year = {2026},\n  url = {https://github.com/kasperfyhn/narrativegraphs}\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! This document explains how to set up a development environment and contribute to <code>narrativegraphs</code>.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/kasperilarsen/narrativegraphs.git\ncd narrativegraphs\n</code></pre> </li> <li> <p>Create a virtual environment and install in editable mode:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -e \".[dev]\"\n</code></pre> </li> <li> <p>Install pre-commit hooks:</p> <pre><code>pre-commit install\n</code></pre> </li> </ol>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>This project uses:</p> <ul> <li><code>ruff</code> for linting and formatting Python code</li> <li>Type hints throughout (checked with <code>mypy</code>)</li> <li>Google-style docstrings</li> <li><code>eslint</code> for linting React/TypeScript code and <code>prettier</code> for formatting </li> <li></li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/your-feature</code></li> <li>Make your changes with tests</li> <li>Run the test suite and linters</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub or reach out directly.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all public modules, classes, and functions in <code>narrativegraphs</code>.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>narrativegraphs/\n\u251c\u2500\u2500 narrativegraph.py        # Core graph classes\n</code></pre>"},{"location":"api/#quick-links","title":"Quick Links","text":"<ul> <li><code>NarrativeGraph</code> \u2014 Main class for creating and querying graphs</li> </ul>"},{"location":"api/graph/","title":"Graph Module","text":"<p>The core module containing the <code>NarrativeGraph</code> class and related data structures.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph","title":"<code>NarrativeGraph</code>","text":"<p>               Bases: <code>BaseGraph</code></p> <p>Full narrative graph with triplet extraction, relations, and co-occurrences.</p> <p>NarrativeGraph extracts subject-predicate-object triplets from text documents and builds both a directed relation graph and an undirected co-occurrence graph.</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>class NarrativeGraph(BaseGraph):\n    \"\"\"Full narrative graph with triplet extraction, relations, and co-occurrences.\n\n    NarrativeGraph extracts subject-predicate-object triplets from text documents\n    and builds both a directed relation graph and an undirected co-occurrence graph.\n    \"\"\"\n\n    def __init__(\n        self,\n        triplet_extractor: TripletExtractor = None,\n        cooccurrence_extractor: CooccurrenceExtractor = None,\n        entity_mapper: Mapper = None,\n        predicate_mapper: Mapper = None,\n        sqlite_db_path: str = None,\n        on_existing_db: Literal[\"stop\", \"overwrite\", \"reuse\"] = \"stop\",\n        n_cpu: int = 1,\n    ):\n        \"\"\"Initialize a NarrativeGraph.\n\n        Args:\n            triplet_extractor: Extractor for subject-predicate-object triplets.\n            cooccurrence_extractor: Extractor for entity co-occurrences.\n            entity_mapper: Mapper for entity normalization.\n            predicate_mapper: Mapper for predicate normalization.\n            sqlite_db_path: Path to SQLite database file. If None, uses in-memory DB.\n            on_existing_db: Behavior when database exists:\n                - \"stop\": Raise error if DB contains data\n                - \"overwrite\": Delete existing DB\n                - \"reuse\": Use existing DB data\n            n_cpu: Number of CPUs for parallel processing (-1 for all).\n        \"\"\"\n        super().__init__(sqlite_db_path, on_existing_db)\n        self._pipeline = Pipeline(\n            self._engine,\n            triplet_extractor=triplet_extractor,\n            cooccurrence_extractor=cooccurrence_extractor,\n            entity_mapper=entity_mapper,\n            predicate_mapper=predicate_mapper,\n            n_cpu=n_cpu,\n        )\n\n    def fit(\n        self,\n        docs: list[str],\n        doc_ids: list[int | str] = None,\n        timestamps: list[datetime | date] = None,\n        categories: (\n            list[str | list[str]]\n            | dict[str, list[str | list[str]]]\n            | list[dict[str, str | list[str]]]\n        ) = None,\n    ) -&gt; \"NarrativeGraph\":\n        \"\"\"\n        Fit a narrative graph from documents. The docs can be accompanied by lists with\n        the same length of IDs, timestamps and categories.\n\n        Args:\n            docs: Required argument, a list of documents as strings.\n            doc_ids: Optional list of document ids. Same length as docs.\n            timestamps: Optional list of document timestamps. Same length as docs.\n            categories: Optional list of document categories. Supports single or\n                multiple categories. A document can have a single or multiple labels\n                per category. See further down for examples.\n\n        Returns:\n            A fitted NarrativeGraph instance.\n\n        \"\"\"\n        self._pipeline.run(\n            docs,\n            doc_ids=doc_ids,\n            timestamps=timestamps,\n            categories=categories,\n        )\n        return self\n\n    @property\n    def predicates_(self) -&gt; pd.DataFrame:\n        \"\"\"Predicates as a pandas DataFrame.\"\"\"\n        return self.predicates.as_df()\n\n    @property\n    def relations_(self) -&gt; pd.DataFrame:\n        \"\"\"Relations as a pandas DataFrame.\"\"\"\n        return self.relations.as_df()\n\n    @property\n    def triplets_(self) -&gt; pd.DataFrame:\n        \"\"\"Triplets as a pandas DataFrame.\"\"\"\n        return self.triplets.as_df()\n\n    @property\n    def relation_graph_(self) -&gt; nx.DiGraph:\n        \"\"\"The full relation graph as a directed NetworkX graph.\"\"\"\n        rg = self.graph.get_graph(\"relation\")\n        g = nx.DiGraph()\n        g.add_nodes_from((n.id, n) for n in rg.nodes)\n        g.add_edges_from((e.from_id, e.to_id, e) for e in rg.edges)\n        return g\n\n    @classmethod\n    def load(cls, file_path: str) -&gt; \"NarrativeGraph\":\n        \"\"\"\n\n        Args:\n            file_path: path to a SQLite database to load a NarrativeGraph from.\n\n        Returns:\n            A NarrativeGraph object\n        \"\"\"\n        return super().load(file_path)  # noqa\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.predicates_","title":"<code>predicates_</code>  <code>property</code>","text":"<p>Predicates as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.relations_","title":"<code>relations_</code>  <code>property</code>","text":"<p>Relations as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.triplets_","title":"<code>triplets_</code>  <code>property</code>","text":"<p>Triplets as a pandas DataFrame.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.relation_graph_","title":"<code>relation_graph_</code>  <code>property</code>","text":"<p>The full relation graph as a directed NetworkX graph.</p>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.__init__","title":"<code>__init__(triplet_extractor=None, cooccurrence_extractor=None, entity_mapper=None, predicate_mapper=None, sqlite_db_path=None, on_existing_db='stop', n_cpu=1)</code>","text":"<p>Initialize a NarrativeGraph.</p> <p>Parameters:</p> Name Type Description Default <code>triplet_extractor</code> <code>TripletExtractor</code> <p>Extractor for subject-predicate-object triplets.</p> <code>None</code> <code>cooccurrence_extractor</code> <code>CooccurrenceExtractor</code> <p>Extractor for entity co-occurrences.</p> <code>None</code> <code>entity_mapper</code> <code>Mapper</code> <p>Mapper for entity normalization.</p> <code>None</code> <code>predicate_mapper</code> <code>Mapper</code> <p>Mapper for predicate normalization.</p> <code>None</code> <code>sqlite_db_path</code> <code>str</code> <p>Path to SQLite database file. If None, uses in-memory DB.</p> <code>None</code> <code>on_existing_db</code> <code>Literal['stop', 'overwrite', 'reuse']</code> <p>Behavior when database exists: - \"stop\": Raise error if DB contains data - \"overwrite\": Delete existing DB - \"reuse\": Use existing DB data</p> <code>'stop'</code> <code>n_cpu</code> <code>int</code> <p>Number of CPUs for parallel processing (-1 for all).</p> <code>1</code> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>def __init__(\n    self,\n    triplet_extractor: TripletExtractor = None,\n    cooccurrence_extractor: CooccurrenceExtractor = None,\n    entity_mapper: Mapper = None,\n    predicate_mapper: Mapper = None,\n    sqlite_db_path: str = None,\n    on_existing_db: Literal[\"stop\", \"overwrite\", \"reuse\"] = \"stop\",\n    n_cpu: int = 1,\n):\n    \"\"\"Initialize a NarrativeGraph.\n\n    Args:\n        triplet_extractor: Extractor for subject-predicate-object triplets.\n        cooccurrence_extractor: Extractor for entity co-occurrences.\n        entity_mapper: Mapper for entity normalization.\n        predicate_mapper: Mapper for predicate normalization.\n        sqlite_db_path: Path to SQLite database file. If None, uses in-memory DB.\n        on_existing_db: Behavior when database exists:\n            - \"stop\": Raise error if DB contains data\n            - \"overwrite\": Delete existing DB\n            - \"reuse\": Use existing DB data\n        n_cpu: Number of CPUs for parallel processing (-1 for all).\n    \"\"\"\n    super().__init__(sqlite_db_path, on_existing_db)\n    self._pipeline = Pipeline(\n        self._engine,\n        triplet_extractor=triplet_extractor,\n        cooccurrence_extractor=cooccurrence_extractor,\n        entity_mapper=entity_mapper,\n        predicate_mapper=predicate_mapper,\n        n_cpu=n_cpu,\n    )\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.fit","title":"<code>fit(docs, doc_ids=None, timestamps=None, categories=None)</code>","text":"<p>Fit a narrative graph from documents. The docs can be accompanied by lists with the same length of IDs, timestamps and categories.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list[str]</code> <p>Required argument, a list of documents as strings.</p> required <code>doc_ids</code> <code>list[int | str]</code> <p>Optional list of document ids. Same length as docs.</p> <code>None</code> <code>timestamps</code> <code>list[datetime | date]</code> <p>Optional list of document timestamps. Same length as docs.</p> <code>None</code> <code>categories</code> <code>list[str | list[str]] | dict[str, list[str | list[str]]] | list[dict[str, str | list[str]]]</code> <p>Optional list of document categories. Supports single or multiple categories. A document can have a single or multiple labels per category. See further down for examples.</p> <code>None</code> <p>Returns:</p> Type Description <code>NarrativeGraph</code> <p>A fitted NarrativeGraph instance.</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>def fit(\n    self,\n    docs: list[str],\n    doc_ids: list[int | str] = None,\n    timestamps: list[datetime | date] = None,\n    categories: (\n        list[str | list[str]]\n        | dict[str, list[str | list[str]]]\n        | list[dict[str, str | list[str]]]\n    ) = None,\n) -&gt; \"NarrativeGraph\":\n    \"\"\"\n    Fit a narrative graph from documents. The docs can be accompanied by lists with\n    the same length of IDs, timestamps and categories.\n\n    Args:\n        docs: Required argument, a list of documents as strings.\n        doc_ids: Optional list of document ids. Same length as docs.\n        timestamps: Optional list of document timestamps. Same length as docs.\n        categories: Optional list of document categories. Supports single or\n            multiple categories. A document can have a single or multiple labels\n            per category. See further down for examples.\n\n    Returns:\n        A fitted NarrativeGraph instance.\n\n    \"\"\"\n    self._pipeline.run(\n        docs,\n        doc_ids=doc_ids,\n        timestamps=timestamps,\n        categories=categories,\n    )\n    return self\n</code></pre>"},{"location":"api/graph/#narrativegraphs.NarrativeGraph.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>path to a SQLite database to load a NarrativeGraph from.</p> required <p>Returns:</p> Type Description <code>NarrativeGraph</code> <p>A NarrativeGraph object</p> Source code in <code>narrativegraphs/graphs.py</code> <pre><code>@classmethod\ndef load(cls, file_path: str) -&gt; \"NarrativeGraph\":\n    \"\"\"\n\n    Args:\n        file_path: path to a SQLite database to load a NarrativeGraph from.\n\n    Returns:\n        A NarrativeGraph object\n    \"\"\"\n    return super().load(file_path)  # noqa\n</code></pre>"},{"location":"api/triplet_extraction/","title":"Extraction Module","text":""},{"location":"api/triplet_extraction/#tripletextractor","title":"TripletExtractor","text":""},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor","title":"<code>TripletExtractor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for triplet extraction algorithms.</p> <p>Triplets are instantiated as Triplet objects that consist of SpanAnnotation objects.</p> <p>Thus, to create a Triplet, you create the</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>class TripletExtractor(ABC):\n    \"\"\"\n    Abstract base class for triplet extraction algorithms.\n\n    Triplets are instantiated as Triplet objects that consist of SpanAnnotation objects.\n\n    Thus, to create a Triplet, you create the\n    \"\"\"\n\n    @abstractmethod\n    def extract(self, text: str) -&gt; list[Triplet]:\n        \"\"\"Single document extraction\n        Args:\n            text: a raw text string\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        pass\n\n    def batch_extract(\n        self, texts: Iterable[str], n_cpu: int = 1, **kwargs\n    ) -&gt; Generator[list[Triplet], None, None]:\n        \"\"\"Multiple-document extraction\n        Args:\n            texts: an iterable of raw text strings; may be a generator, so be mindful\n                of consuming items\n            n_cpu: number of CPUs to use\n            **kwargs: other keyword arguments for your own class\n\n        Returns:\n            should yield triplets per text in the same order as texts iterable\n\n        \"\"\"\n        for text in texts:\n            yield self.extract(text)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor.extract","title":"<code>extract(text)</code>  <code>abstractmethod</code>","text":"<p>Single document extraction Args:     text: a raw text string</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>@abstractmethod\ndef extract(self, text: str) -&gt; list[Triplet]:\n    \"\"\"Single document extraction\n    Args:\n        text: a raw text string\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.common.TripletExtractor.batch_extract","title":"<code>batch_extract(texts, n_cpu=1, **kwargs)</code>","text":"<p>Multiple-document extraction Args:     texts: an iterable of raw text strings; may be a generator, so be mindful         of consuming items     n_cpu: number of CPUs to use     **kwargs: other keyword arguments for your own class</p> <p>Returns:</p> Type Description <code>None</code> <p>should yield triplets per text in the same order as texts iterable</p> Source code in <code>narrativegraphs/nlp/triplets/common.py</code> <pre><code>def batch_extract(\n    self, texts: Iterable[str], n_cpu: int = 1, **kwargs\n) -&gt; Generator[list[Triplet], None, None]:\n    \"\"\"Multiple-document extraction\n    Args:\n        texts: an iterable of raw text strings; may be a generator, so be mindful\n            of consuming items\n        n_cpu: number of CPUs to use\n        **kwargs: other keyword arguments for your own class\n\n    Returns:\n        should yield triplets per text in the same order as texts iterable\n\n    \"\"\"\n    for text in texts:\n        yield self.extract(text)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor","title":"<code>SpacyTripletExtractor</code>","text":"<p>               Bases: <code>TripletExtractor</code></p> <p>Base class for implementing triplet extraction based on spaCy docs.</p> <p>Override <code>extract_triplets_from_sent</code> for extracting triplets sentence by sentence.</p> <p>Override <code>extract_triplets_from_doc</code> for extracting with the full Doc context.</p> <p>The <code>SpanAnnotation</code> objects of <code>Triplet</code> objects can conveniently be created from a spaCy <code>Span</code> object with <code>SpanAnnotation.from_span()</code>.</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>class SpacyTripletExtractor(TripletExtractor):\n    \"\"\"Base class for implementing triplet extraction based on spaCy docs.\n\n    Override `extract_triplets_from_sent` for extracting triplets sentence by sentence.\n\n    Override `extract_triplets_from_doc` for extracting with the full Doc context.\n\n    The `SpanAnnotation` objects of `Triplet` objects can conveniently be created from\n    a spaCy `Span` object with `SpanAnnotation.from_span()`.\n    \"\"\"\n\n    def __init__(\n        self, model_name: str = None, split_sentence_on_double_line_break: bool = True\n    ):\n        \"\"\"\n        Args:\n            model_name: name of the spaCy model to use\n            split_sentence_on_double_line_break: adds extra sentence boundaries on\n                double line breaks (\"\\n\\n\")\n        \"\"\"\n        if model_name is None:\n            model_name = \"en_core_web_sm\"\n        self.nlp = ensure_spacy_model(model_name)\n        if split_sentence_on_double_line_break:\n            self.nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")\n\n    @abstractmethod\n    def extract_triplets_from_sent(self, sent: Span) -&gt; list[Triplet]:\n        \"\"\"Extract triplets from a SpaCy sentence.\n        Args:\n            sent: A SpaCy Span object representing the whole sentence\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        pass\n\n    def extract_triplets_from_doc(self, doc: Doc) -&gt; list[Triplet]:\n        \"\"\"Extract triplets from a Doc\n        Args:\n            doc: A SpaCy Doc object\n\n        Returns:\n            extracted triplets\n        \"\"\"\n        triplets = []\n        for sent in doc.sents:\n            sent_triplets = self.extract_triplets_from_sent(sent)\n            if sent_triplets is not None:\n                triplets.extend(sent_triplets)\n        return triplets\n\n    def extract(self, text: str) -&gt; list[Triplet]:\n        text = self.nlp(text)\n        return self.extract_triplets_from_doc(text)\n\n    def batch_extract(\n        self, texts: list[str], n_cpu: int = 1, batch_size: int = None\n    ) -&gt; Generator[list[Triplet], None, None]:\n        if batch_size is None:\n            batch_size = calculate_batch_size(texts, n_cpu)\n        if n_cpu &gt; 1:\n            _logger.info(\n                \"Using multiple CPU cores.Progress bars may stand still at first.\"\n            )\n        for doc in self.nlp.pipe(texts, n_process=n_cpu, batch_size=batch_size):\n            yield self.extract_triplets_from_doc(doc)\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.__init__","title":"<code>__init__(model_name=None, split_sentence_on_double_line_break=True)</code>","text":"<pre><code>    Args:\n        model_name: name of the spaCy model to use\n        split_sentence_on_double_line_break: adds extra sentence boundaries on\n            double line breaks (\"\n</code></pre> <p>\")</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>def __init__(\n    self, model_name: str = None, split_sentence_on_double_line_break: bool = True\n):\n    \"\"\"\n    Args:\n        model_name: name of the spaCy model to use\n        split_sentence_on_double_line_break: adds extra sentence boundaries on\n            double line breaks (\"\\n\\n\")\n    \"\"\"\n    if model_name is None:\n        model_name = \"en_core_web_sm\"\n    self.nlp = ensure_spacy_model(model_name)\n    if split_sentence_on_double_line_break:\n        self.nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.extract_triplets_from_sent","title":"<code>extract_triplets_from_sent(sent)</code>  <code>abstractmethod</code>","text":"<p>Extract triplets from a SpaCy sentence. Args:     sent: A SpaCy Span object representing the whole sentence</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>@abstractmethod\ndef extract_triplets_from_sent(self, sent: Span) -&gt; list[Triplet]:\n    \"\"\"Extract triplets from a SpaCy sentence.\n    Args:\n        sent: A SpaCy Span object representing the whole sentence\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/triplet_extraction/#narrativegraphs.nlp.triplets.spacy.common.SpacyTripletExtractor.extract_triplets_from_doc","title":"<code>extract_triplets_from_doc(doc)</code>","text":"<p>Extract triplets from a Doc Args:     doc: A SpaCy Doc object</p> <p>Returns:</p> Type Description <code>list[Triplet]</code> <p>extracted triplets</p> Source code in <code>narrativegraphs/nlp/triplets/spacy/common.py</code> <pre><code>def extract_triplets_from_doc(self, doc: Doc) -&gt; list[Triplet]:\n    \"\"\"Extract triplets from a Doc\n    Args:\n        doc: A SpaCy Doc object\n\n    Returns:\n        extracted triplets\n    \"\"\"\n    triplets = []\n    for sent in doc.sents:\n        sent_triplets = self.extract_triplets_from_sent(sent)\n        if sent_triplets is not None:\n            triplets.extend(sent_triplets)\n    return triplets\n</code></pre>"},{"location":"developer/nlp-layer/","title":"NLP Layer","text":"<p>This guide provides an overview of the NLP layer in <code>narrativegraphs/nlp/</code>.</p>"},{"location":"developer/nlp-layer/#pipelines","title":"Pipelines","text":"<p>The NLP layer provides two main pipelines that orchestrate the full extraction workflow:</p> Pipeline Purpose Extracts Pipeline Full narrative graph extraction Triplets + cooccurrences CooccurrencePipeline Simpler cooccurrence-only extraction Entities + cooccurrences <p>Both pipelines handle:</p> <ol> <li>Adding documents to the database</li> <li>Extracting and storing annotations</li> <li>Mapping surface forms to canonical entities/predicates</li> <li>Calculating statistics</li> </ol>"},{"location":"developer/nlp-layer/#pipeline-full","title":"Pipeline (Full)","text":"<p>Uses a <code>TripletExtractor</code> to extract subject-predicate-object triplets, then derives entities from those triplets. Also extracts cooccurrences between entities.</p> <p>Default components:</p> <ul> <li>Triplet extraction: <code>DependencyGraphExtractor</code></li> <li>Cooccurrence extraction: <code>ChunkCooccurrenceExtractor</code></li> <li>Entity mapping: <code>SubgramStemmingMapper(\"noun\")</code></li> <li>Predicate mapping: <code>SubgramStemmingMapper(\"verb\")</code></li> </ul>"},{"location":"developer/nlp-layer/#cooccurrencepipeline","title":"CooccurrencePipeline","text":"<p>Uses an <code>EntityExtractor</code> directly to find entities, then extracts cooccurrences between them. Skips triplet extraction entirely.</p> <p>Default components:</p> <ul> <li>Entity extraction: <code>SpacyEntityExtractor</code></li> <li>Cooccurrence extraction: <code>ChunkCooccurrenceExtractor</code></li> <li>Entity mapping: <code>SubgramStemmingMapper(\"noun\")</code></li> </ul>"},{"location":"developer/nlp-layer/#extraction-components","title":"Extraction Components","text":""},{"location":"developer/nlp-layer/#entity-extraction-entities","title":"Entity Extraction (<code>entities/</code>)","text":"Class Description EntityExtractor Abstract base class SpacyEntityExtractor Uses spaCy NER and/or noun chunks with configurable length filters <p><code>SpacyEntityExtractor</code> features:</p> <ul> <li>Configurable NER and noun chunk extraction</li> <li>Token length filtering (min/max tokens)</li> <li>Greedy non-overlapping span selection (NER takes priority)</li> <li>Pronoun filtering</li> <li>Parallel batch processing</li> </ul>"},{"location":"developer/nlp-layer/#triplet-extraction-triplets","title":"Triplet Extraction (<code>triplets/</code>)","text":"Class Description TripletExtractor Abstract base class DependencyGraphExtractor Extracts triplets from spaCy dependency parses <p>Triplets consist of:</p> <ul> <li><code>subj</code>: Subject entity (SpanAnnotation)</li> <li><code>pred</code>: Predicate/verb (SpanAnnotation)</li> <li><code>obj</code>: Object entity (SpanAnnotation)</li> <li><code>context</code>: Optional sentence context (AnnotationContext)</li> </ul>"},{"location":"developer/nlp-layer/#cooccurrence-extraction-tuplets","title":"Cooccurrence Extraction (<code>tuplets/</code>)","text":"Class Description CooccurrenceExtractor Abstract base class ChunkCooccurrenceExtractor Sentence-based windowed cooccurrences DocumentCooccurrenceExtractor All entity pairs within a document <p><code>ChunkCooccurrenceExtractor</code> features:</p> <ul> <li>Configurable sentence window size</li> <li>Custom boundary patterns (regex or callable)</li> <li>Sentence-level context capture</li> </ul> <p>Tuplets consist of:</p> <ul> <li><code>entity_one</code>, <code>entity_two</code>: Entity pair (SpanAnnotation)</li> <li><code>context</code>: Optional context window (AnnotationContext)</li> </ul>"},{"location":"developer/nlp-layer/#mapping-components-mapping","title":"Mapping Components (<code>mapping/</code>)","text":"<p>Mappers normalize surface forms to canonical labels, creating a <code>dict[str, str]</code> mapping.</p> Class Description Mapper Abstract base class StemmingMapper Groups by Porter stemmed form SubgramStemmingMapper Stemming + subgram matching for head words <p><code>SubgramStemmingMapper</code> (default):</p> <ul> <li>First applies stemming normalization</li> <li>Then matches shorter forms to longer ones containing them</li> <li>Configurable for nouns or verbs via <code>head_word_type</code></li> <li>Ranking by shortest label or most frequent</li> </ul>"},{"location":"developer/nlp-layer/#supporting-components","title":"Supporting Components","text":""},{"location":"developer/nlp-layer/#common-utilities-common","title":"Common Utilities (<code>common/</code>)","text":"Module Purpose annotation.py Data models: <code>SpanAnnotation</code>, <code>AnnotationContext</code> spacy.py spaCy utilities: model loading, batch size calculation, span filtering transformcategories.py Normalizes various category input formats <p><code>SpanAnnotation</code> represents a text span with:</p> <ul> <li><code>text</code>: Surface form</li> <li><code>start_char</code>, <code>end_char</code>: Character offsets</li> <li><code>normalized_text</code>: Optional lemma</li> </ul>"},{"location":"developer/nlp-layer/#filtering-filtering","title":"Filtering (<code>filtering/</code>)","text":"Class Description BigramFilter PMI-based bigram filtering for quality control <p><code>BigramFilter</code> can be used to filter out low-quality multi-word spans based on bigram co-occurrence statistics.</p>"},{"location":"developer/nlp-layer/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>Pipeline / CooccurrencePipeline\n    \u2502\n    \u251c\u2500\u2500 Document ingestion\n    \u2502\n    \u251c\u2500\u2500 Extraction\n    \u2502   \u251c\u2500\u2500 TripletExtractor \u2500\u2500\u25ba Triplet (subj, pred, obj)\n    \u2502   \u2502   \u2514\u2500\u2500 DependencyGraphExtractor (spaCy)\n    \u2502   \u2502\n    \u2502   \u251c\u2500\u2500 EntityExtractor \u2500\u2500\u25ba SpanAnnotation\n    \u2502   \u2502   \u2514\u2500\u2500 SpacyEntityExtractor (NER + noun chunks)\n    \u2502   \u2502\n    \u2502   \u2514\u2500\u2500 CooccurrenceExtractor \u2500\u2500\u25ba Tuplet (entity_one, entity_two)\n    \u2502       \u251c\u2500\u2500 ChunkCooccurrenceExtractor (sentence window)\n    \u2502       \u2514\u2500\u2500 DocumentCooccurrenceExtractor (all pairs)\n    \u2502\n    \u251c\u2500\u2500 Mapping\n    \u2502   \u2514\u2500\u2500 Mapper \u2500\u2500\u25ba dict[str, str]\n    \u2502       \u251c\u2500\u2500 StemmingMapper\n    \u2502       \u2514\u2500\u2500 SubgramStemmingMapper (default)\n    \u2502\n    \u2514\u2500\u2500 Stats calculation (via service layer)\n</code></pre>"},{"location":"developer/nlp-layer/#extensibility","title":"Extensibility","text":"<p>All extraction and mapping components use abstract base classes, making it easy to implement custom:</p> <ul> <li>Entity extractors (e.g., domain-specific NER)</li> <li>Triplet extractors (e.g., rule-based, LLM-based)</li> <li>Cooccurrence extractors (e.g., paragraph-level, custom boundaries)</li> <li>Mappers (e.g., embedding-based clustering, knowledge base linking)</li> </ul>"},{"location":"developer/orm-structure/","title":"ORM Structure","text":"<p>This guide provides an overview of the ORM structure in <code>narrativegraphs/db/</code>.</p>"},{"location":"developer/orm-structure/#core-concepts","title":"Core Concepts","text":"<p>The data model supports two graph paradigms:</p> Graph Type Primary Annotations Has Relations/Predicates NarrativeGraph Triplets (subject-predicate-object) Yes CooccurrenceGraph Tuplets (entity-entity pairs) No"},{"location":"developer/orm-structure/#annotation-types","title":"Annotation Types","text":"<p>Annotation ORMs store text extractions and have a direct <code>doc_id</code> reference.</p>"},{"location":"developer/orm-structure/#tripletorm-tripletspy","title":"TripletOrm (<code>triplets.py</code>)","text":"<p>Represents a subject-predicate-object extraction from text.</p> <ul> <li>Has: <code>doc_id</code>, <code>subject_id</code>, <code>predicate_id</code>, <code>object_id</code>, <code>relation_id</code>, <code>cooccurrence_id</code></li> <li>Stores span positions and text for subject, predicate, and object</li> <li>Mixes in <code>AnnotationMixin</code> (provides <code>doc_id</code>, <code>timestamp</code>, <code>document</code> relationship)</li> </ul>"},{"location":"developer/orm-structure/#tupletorm-tupletspy","title":"TupletOrm (<code>tuplets.py</code>)","text":"<p>Represents an entity-entity cooccurrence extraction.</p> <ul> <li>Has: <code>doc_id</code>, <code>entity_one_id</code>, <code>entity_two_id</code>, <code>cooccurrence_id</code></li> <li>Stores span positions and text for both entities</li> <li>Mixes in <code>AnnotationMixin</code></li> </ul>"},{"location":"developer/orm-structure/#entityoccurrenceorm-entityoccurrencespy","title":"EntityOccurrenceOrm (<code>entityoccurrences.py</code>)","text":"<p>Represents a single entity mention/occurrence in text.</p> <ul> <li>Has: <code>doc_id</code>, <code>entity_id</code>, <code>span_start</code>, <code>span_end</code>, <code>span_text</code></li> <li>Relationships: <code>entity</code> (\u2192 EntityOrm), <code>document</code> (\u2192 DocumentOrm)</li> <li>Mixes in <code>AnnotationMixin</code></li> <li>Used by EntityOrm to derive <code>alt_labels</code> (alternative surface forms)</li> </ul>"},{"location":"developer/orm-structure/#higher-level-orms","title":"Higher-Level ORMs","text":"<p>These ORMs represent canonical/deduplicated concepts backed by annotations. All mix in <code>AnnotationBackedTextStatsMixin</code> which provides:</p> <ul> <li>Stats columns: <code>frequency</code>, <code>doc_frequency</code>, <code>spread</code>, <code>adjusted_tf_idf</code>, <code>first_occurrence</code>, <code>last_occurrence</code></li> <li><code>_annotations</code> property (abstract, returns backing triplets/tuplets)</li> <li><code>doc_ids</code> property (derived from <code>_annotations</code>)</li> </ul>"},{"location":"developer/orm-structure/#entityorm-entitiespy","title":"EntityOrm (<code>entities.py</code>)","text":"<p>Canonical entity (e.g., \"Microsoft\", \"Satya Nadella\").</p> <ul> <li>Relationships:</li> <li><code>occurrences</code> \u2192 EntityOccurrenceOrm (all mentions of this entity)</li> <li><code>subject_triplets</code> / <code>object_triplets</code> \u2192 <code>triplets</code> property</li> <li><code>_entity_one_tuplets</code> / <code>_entity_two_tuplets</code> \u2192 <code>tuplets</code> property</li> <li><code>subject_relations</code> / <code>object_relations</code> \u2192 <code>relations</code> property</li> <li><code>_entity_one_cooccurrences</code> / <code>_entity_two_cooccurrences</code> \u2192 <code>cooccurrences</code> property</li> <li><code>_annotations</code> returns <code>triplets + tuplets</code> (union for both graph types)</li> <li>Has <code>alt_labels</code> hybrid property (derived from <code>occurrences</code> span texts)</li> </ul>"},{"location":"developer/orm-structure/#predicateorm-predicatespy","title":"PredicateOrm (<code>predicates.py</code>)","text":"<p>Canonical predicate/verb (e.g., \"acquired\", \"announced\").</p> <ul> <li>Relationships: <code>triplets</code>, <code>relations</code></li> <li><code>_annotations</code> returns <code>triplets</code></li> <li>Has <code>alt_labels</code> hybrid property</li> </ul>"},{"location":"developer/orm-structure/#relationorm-relationspy","title":"RelationOrm (<code>relations.py</code>)","text":"<p>Canonical relation tuple: (subject_entity, predicate, object_entity).</p> <ul> <li>Has: <code>subject_id</code>, <code>predicate_id</code>, <code>object_id</code>, <code>significance</code></li> <li>Relationships: <code>subject</code>, <code>predicate</code>, <code>object</code>, <code>triplets</code></li> <li><code>_annotations</code> returns <code>triplets</code></li> <li>Has <code>alt_labels</code> hybrid property</li> </ul>"},{"location":"developer/orm-structure/#cooccurrenceorm-cooccurrencespy","title":"CooccurrenceOrm (<code>cooccurrences.py</code>)","text":"<p>Canonical cooccurrence: (entity_one, entity_two) where <code>entity_one_id &lt;= entity_two_id</code>.</p> <ul> <li>Has: <code>entity_one_id</code>, <code>entity_two_id</code>, <code>pmi</code></li> <li>Relationships: <code>entity_one</code>, <code>entity_two</code>, <code>tuplets</code></li> <li><code>_annotations</code> returns <code>tuplets</code></li> </ul>"},{"location":"developer/orm-structure/#documentorm-documentspy","title":"DocumentOrm (<code>documents.py</code>)","text":"<p>Source document with <code>text</code>, <code>str_id</code>, <code>timestamp</code>.</p> <ul> <li>Relationships: <code>triplets</code>, <code>tuplets</code>, <code>entity_occurrences</code></li> <li>Has categories via <code>CategorizableMixin</code></li> </ul>"},{"location":"developer/orm-structure/#mixins-commonpy-documentspy","title":"Mixins (<code>common.py</code>, <code>documents.py</code>)","text":"Mixin Purpose CategorizableMixin Provides category support CategoryMixin Base for category tables (e.g., <code>EntityCategory</code>) HasAltLabels For ORMs with alternative surface forms AnnotationMixin For triplets/tuplets (provides <code>doc_id</code>, <code>document</code> relationship) AnnotationBackedTextStatsMixin For higher-level ORMs (stats + <code>doc_ids</code>)"},{"location":"developer/orm-structure/#relationship-diagram","title":"Relationship Diagram","text":"<pre><code>DocumentOrm\n    \u2502\n    \u251c\u2500\u2500 triplets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba TripletOrm \u25c4\u2500\u2500 subject/object \u2500\u2500 EntityOrm\n    \u2502                            \u2502                              \u2502\n    \u2502                            \u251c\u2500\u2500 predicate \u2500\u2500 PredicateOrm  \u2502\n    \u2502                            \u2502                    \u2502         \u2502\n    \u2502                            \u2514\u2500\u2500 relation \u2500\u2500\u2500 RelationOrm \u25c4\u2500\u2518\n    \u2502                                                \u2502\n    \u251c\u2500\u2500 tuplets \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba TupletOrm \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500 entity_one/two \u2500\u2500 EntityOrm\n    \u2502                            \u2502                   \u2502\n    \u2502                            \u2514\u2500\u2500 cooccurrence \u2500\u2500 CooccurrenceOrm\n    \u2502\n    \u2514\u2500\u2500 entity_occurrences \u2500\u25ba EntityOccurrenceOrm \u25c4\u2500\u2500 entity \u2500\u2500 EntityOrm\n</code></pre>"},{"location":"developer/server-layer/","title":"Server Layer","text":"<p>This guide provides an overview of the server layer in <code>narrativegraphs/server/</code>.</p>"},{"location":"developer/server-layer/#overview","title":"Overview","text":"<p>The server layer provides a FastAPI-based REST API for querying narrative graph data, plus a built-in visualization frontend.</p>"},{"location":"developer/server-layer/#components","title":"Components","text":"Component Purpose app.py FastAPI application with lifespan management and route registration backgroundserver.py Utility for running the server in notebooks or background requests.py Pydantic request models for API endpoints routes/ API route handlers organized by entity type static/ Pre-built frontend visualization assets"},{"location":"developer/server-layer/#fastapi-application-apppy","title":"FastAPI Application (<code>app.py</code>)","text":"<p>The main application handles:</p> <ul> <li>Database engine initialization (from <code>DB_PATH</code> env var or provided engine)</li> <li>QueryService instantiation for all routes</li> <li>CORS middleware configuration</li> <li>Static file serving for the visualization frontend</li> <li>Custom exception handling for <code>EntryNotFoundError</code></li> </ul>"},{"location":"developer/server-layer/#running-the-server","title":"Running the Server","text":"<p>Standalone:</p> <pre><code># Set DB_PATH environment variable, then:\nuvicorn narrativegraphs.server.app:app --host localhost --port 8001\n</code></pre> <p>In notebooks:</p> <pre><code>from narrativegraphs.server import BackgroundServer\n\nserver = BackgroundServer(db_engine, port=8001)\nserver.start()  # blocking\n# or\nserver.start(block=False)  # background\nserver.show_iframe()  # display in notebook\n</code></pre>"},{"location":"developer/server-layer/#api-routes","title":"API Routes","text":"<p>Routes are organized by entity type in the <code>routes/</code> directory:</p> Router Prefix Purpose graph <code>/graph</code> Graph queries, bounds, community detection entities <code>/entities</code> Entity lookup, search, related documents documents <code>/docs</code> Document retrieval relations <code>/relations</code> Relation lookup and related documents cooccurrences <code>/cooccurrences</code> Cooccurrence lookup and related documents <p>All routes use the shared <code>QueryService</code> via FastAPI dependency injection. See the route files for current endpoint details.</p>"},{"location":"developer/server-layer/#backgroundserver","title":"BackgroundServer","text":"<p>Utility class for running the server programmatically:</p> <ul> <li>Supports blocking and non-blocking modes</li> <li>Graceful shutdown handling</li> <li><code>show_iframe()</code> for Jupyter notebook display</li> </ul>"},{"location":"developer/server-layer/#static-frontend","title":"Static Frontend","text":"<p>The <code>static/</code> directory contains pre-built frontend assets (React app) that provide:</p> <ul> <li>Interactive graph visualization</li> <li>Entity/relation exploration</li> <li>Document viewer with highlighted annotations</li> <li>Filtering controls</li> </ul> <p>The frontend is served at the root URL when the server is running.</p>"},{"location":"developer/server-layer/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>FastAPI App (app.py)\n    \u2502\n    \u251c\u2500\u2500 Lifespan: Initialize DB engine + QueryService\n    \u2502\n    \u251c\u2500\u2500 Middleware: CORS\n    \u2502\n    \u251c\u2500\u2500 Routes (routes/)\n    \u2502   \u251c\u2500\u2500 graph \u2500\u2500\u2500\u2500\u2500\u2500\u25ba GraphService\n    \u2502   \u251c\u2500\u2500 entities \u2500\u2500\u2500\u25ba EntityService\n    \u2502   \u251c\u2500\u2500 documents \u2500\u2500\u25ba DocService\n    \u2502   \u251c\u2500\u2500 relations \u2500\u2500\u25ba RelationService\n    \u2502   \u2514\u2500\u2500 cooccurrences \u25ba CooccurrenceService\n    \u2502\n    \u2514\u2500\u2500 Static Files: Frontend visualization\n\nBackgroundServer\n    \u2502\n    \u2514\u2500\u2500 Wraps FastAPI app for notebook/background usage\n</code></pre>"},{"location":"developer/service-layer/","title":"Service Layer","text":"<p>This guide provides an overview of the service layer in <code>narrativegraphs/service/</code>.</p>"},{"location":"developer/service-layer/#core-services","title":"Core Services","text":"<p>The service layer has two main entry points:</p> Service Purpose QueryService Read/query operations on the database PopulationService Write operations (adding documents, annotations, mapping) <p>Both extend <code>DbService</code> which provides thread-safe session management via <code>get_session_context()</code>.</p>"},{"location":"developer/service-layer/#queryservice","title":"QueryService","text":"<p>Main entry point for reading data. Composes sub-services for each entity type:</p> Sub-service Entity Type Key Capabilities <code>documents</code> DocumentOrm Retrieve docs with optional eager-loaded annotations <code>entities</code> EntityOrm Search, lookup, get associated doc IDs <code>relations</code> RelationOrm Query relations between entities <code>predicates</code> PredicateOrm Query predicates <code>cooccurrences</code> CooccurrenceOrm Query cooccurrences between entities <code>triplets</code> TripletOrm Query triplet annotations <code>tuplets</code> TupletOrm Query tuplet annotations <code>graph</code> Graph operations Subgraph extraction, expansion, community detection <p>All sub-services extend <code>OrmAssociatedService</code> and provide standard methods for DataFrame export, single/multiple record retrieval, plus entity-specific queries.</p>"},{"location":"developer/service-layer/#populationservice","title":"PopulationService","text":"<p>Main entry point for populating the database. Handles:</p> <ol> <li> <p>Document ingestion - Bulk insert documents with metadata (IDs, timestamps, categories)</p> </li> <li> <p>Annotation ingestion (two-phase):</p> </li> <li> <p>First add entity occurrences, get a lookup dict</p> </li> <li> <p>Then add triplets/tuplets referencing occurrences via the lookup</p> </li> <li> <p>Mapping to canonical entities - Map annotations to deduplicated entities, predicates, relations, and cooccurrences using provided mapping dictionaries</p> </li> </ol>"},{"location":"developer/service-layer/#supporting-services","title":"Supporting Services","text":""},{"location":"developer/service-layer/#statscalculator-statspy","title":"StatsCalculator (<code>stats.py</code>)","text":"<p>Computes aggregate statistics after population is complete:</p> <ul> <li>Entity/predicate/relation/cooccurrence frequency and doc_frequency</li> <li>Spread, adjusted TF-IDF, first/last occurrence timestamps</li> <li>Relation significance scores</li> <li>Cooccurrence PMI values</li> <li>Category propagation from documents to higher-level entities</li> </ul>"},{"location":"developer/service-layer/#graphservice-graphpy","title":"GraphService (<code>graph.py</code>)","text":"<p>Specialized service for graph operations:</p> <ul> <li>Subgraph extraction - Get graph for specific entity IDs</li> <li>Expansion - Expand from focus entities to connected neighbors</li> <li>Community detection - Louvain, k-clique, or connected components algorithms</li> </ul> <p>Supports two connection types: <code>\"relation\"</code> (directed, with predicates) and <code>\"cooccurrence\"</code> (undirected pairs).</p>"},{"location":"developer/service-layer/#caches-cachepy","title":"Caches (<code>cache.py</code>)","text":"<p>Used internally by <code>PopulationService</code> for efficient bulk mapping:</p> Cache Purpose EntityCache Maps surface forms to canonical entities PredicateCache Maps predicate texts to canonical predicates CooccurrenceCache Creates/retrieves cooccurrence pairs RelationCache Creates/retrieves relation triples"},{"location":"developer/service-layer/#filter-functions-filterpy","title":"Filter Functions (<code>filter.py</code>)","text":"<p>Builds SQLAlchemy conditions for graph queries. Supports filtering by:</p> <ul> <li>Date range (first/last occurrence)</li> <li>Frequency and doc_frequency bounds</li> <li>Categories</li> <li>Entity blacklist</li> </ul>"},{"location":"developer/service-layer/#base-classes-commonpy","title":"Base Classes (<code>common.py</code>)","text":"Class Purpose DbService Thread-safe session management SubService Base for services sharing session context OrmAssociatedService Base for services tied to a specific ORM (provides <code>as_df</code>, <code>get_single</code>, <code>get_multiple</code>)"},{"location":"developer/service-layer/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>QueryService (read)                    PopulationService (write)\n    \u2502                                          \u2502\n    \u251c\u2500\u2500 documents                              \u251c\u2500\u2500 add documents\n    \u251c\u2500\u2500 entities                               \u251c\u2500\u2500 add entity occurrences\n    \u251c\u2500\u2500 relations                              \u251c\u2500\u2500 add triplets / tuplets\n    \u251c\u2500\u2500 predicates                             \u2514\u2500\u2500 map to canonical entities\n    \u251c\u2500\u2500 cooccurrences                                  \u2502\n    \u251c\u2500\u2500 triplets                                       \u2514\u2500\u2500 Uses Caches\n    \u251c\u2500\u2500 tuplets                                            \u251c\u2500\u2500 EntityCache\n    \u2514\u2500\u2500 graph \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u251c\u2500\u2500 PredicateCache\n                               \u2502                           \u251c\u2500\u2500 CooccurrenceCache\n                               \u2514\u2500\u2500 Uses filter.py          \u2514\u2500\u2500 RelationCache\n\n                           StatsCalculator\n                                \u2502\n                                \u2514\u2500\u2500 calculate_stats() after population\n</code></pre>"},{"location":"developer/visualizer/","title":"Visualizer Frontend","text":"<p>This guide provides an overview of the visualizer in <code>visualizer/</code>.</p>"},{"location":"developer/visualizer/#overview","title":"Overview","text":"<p>A React/TypeScript single-page application that provides interactive visualization of narrative graphs. Built with Create React App and uses react-graph-vis for graph rendering.</p>"},{"location":"developer/visualizer/#project-structure","title":"Project Structure","text":"<pre><code>visualizer/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 components/     # React components\n\u2502   \u251c\u2500\u2500 contexts/       # React context providers\n\u2502   \u251c\u2500\u2500 hooks/          # Custom React hooks\n\u2502   \u251c\u2500\u2500 reducers/       # State reducers\n\u2502   \u251c\u2500\u2500 services/       # API client services\n\u2502   \u2514\u2500\u2500 types/          # TypeScript type definitions\n\u251c\u2500\u2500 public/             # Static assets\n\u2514\u2500\u2500 build/              # Production build output\n</code></pre>"},{"location":"developer/visualizer/#architecture","title":"Architecture","text":""},{"location":"developer/visualizer/#context-providers","title":"Context Providers","text":"<p>The app uses React Context for state management, nested in this order:</p> Context Purpose ServiceContext API service instances (graph, docs, entities, etc.) GraphOptionsContext Visualization options (layout, styling) GraphQueryContext Current query state and filters SelectionContext Currently selected nodes/edges"},{"location":"developer/visualizer/#services-services","title":"Services (<code>services/</code>)","text":"<p>Client-side services that mirror the backend API:</p> Service Backend Route GraphService <code>/graph</code> DocService <code>/docs</code> EntityService <code>/entities</code> RelationService <code>/relations</code> CooccurrenceService <code>/cooccurrences</code> <p>Services auto-detect the API URL based on environment (localhost dev vs production).</p>"},{"location":"developer/visualizer/#components-components","title":"Components (<code>components/</code>)","text":"Directory Purpose graph/ Main graph viewer, sidebar, controls inspector/ Detail panels for selected items (docs, entity info) common/ Shared UI components (panels, entity chips, inputs)"},{"location":"developer/visualizer/#reducers-reducers","title":"Reducers (<code>reducers/</code>)","text":"<p>Complex state management for:</p> <ul> <li>Graph filter state</li> <li>Graph query state</li> <li>Navigation history</li> </ul>"},{"location":"developer/visualizer/#development","title":"Development","text":"<pre><code>cd visualizer\nnpm install\nnpm start        # Dev server on port 3000\nnpm run build    # Production build\n</code></pre> <p>The dev server proxies API requests to <code>localhost:8001</code> (the backend).</p>"},{"location":"developer/visualizer/#deployment","title":"Deployment","text":"<p>The production build (<code>npm run build</code>) outputs to <code>build/</code>, which is copied to <code>narrativegraphs/server/static/</code> to be served by the FastAPI backend.</p>"},{"location":"developer/visualizer/#key-dependencies","title":"Key Dependencies","text":"<ul> <li>react-graph-vis - Graph visualization</li> <li>react-router-dom - Client-side routing</li> <li>lucide-react - Icons</li> <li>draft-js - Rich text editing (for document display)</li> </ul>"},{"location":"getting-started/adding-metadata/","title":"Metadata","text":"<p>The <code>NarrativeGraph.fit()</code> method takes a few extra optional parameters that serve as metadata for your docs. These are document IDs, timestamps and categories.</p> <p>If available for your data, it can be valuable. Timestamps and categories will give some more options for slicing the graph data in the visualizer. IDs are mostly a reference point if you are looking for a specific document.</p> <p>They should be served as lists of the same length.</p> <pre><code>from narrativegraphs import NarrativeGraph\nfrom datetime import date\n\ndocs: list[str] = [...]  # your list of documents\ndoc_ids: list[int] = [...]  # your list of document IDs\ntimestamps: list[date] = [...]  # your list of dates\ncategories: list[str] = [...]  # your list of categories as a string\nmodel = NarrativeGraph().fit(\n    docs,\n    doc_ids=doc_ids,\n    timestamps=timestamps,\n    categories=categories\n)\nmodel.serve_visualizer()\n</code></pre>"},{"location":"getting-started/create-and-inspect/","title":"Create and inspect","text":"<p>The basic workflow of creating and inspecting a narrative graph is:</p> <ol> <li>Import <code>narrativegraphs</code>.</li> <li>Prepare your documents as a list of strings.</li> <li>Initialize a model and fit it on your docs.</li> <li>Serve the visualizer, follow the link, and inspect your docs visually.</li> </ol> <pre><code>from narrativegraphs import NarrativeGraph\n\ndocs: list[str] = [...]  # your list of documents\nmodel = NarrativeGraph().fit(docs)\nmodel.serve_visualizer()\n</code></pre> <p>Open the link in your terminal to explore the graph in your browser:</p> <p></p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (Recommended)","text":"<pre><code>pip install narrativegraphs\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>For the latest development version:</p> <pre><code>pip install git+https://github.com/kasperfyhn/narrativegraphs.git\n</code></pre> <p>Or clone and install in editable mode for development:</p> <pre><code>git clone https://github.com/kasperfyhn/narrativegraphs.git\ncd narrativegraphs\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#setting-up-spacy-models","title":"Setting Up spaCy Models","text":"<p>After installation, download the spaCy model(s) you need:</p> <pre><code>python -m spacy download en_core_web_sm  # English, small\npython -m spacy download da_core_news_sm  # Danish, small\n</code></pre>"},{"location":"getting-started/save-and-load/","title":"Saving and loading","text":"<p>A model can be saved and loaded for later use, so you do not have to re-process documents every time.</p> <p>To save:</p> <pre><code>model.save_to_file(\"my_model.db\")\n</code></pre> <p>To load it again</p> <pre><code>NarrativeGraph.load(\"my_model.db\")\n</code></pre> <p>A handy codeblock is: <pre><code>from narrativegraphs import NarrativeGraph\nimport os.path\n\nmodel_name = \"my_model.db\"\ndocs: list[str] = [...]  # your list of documents\n\nif os.path.exists(model_name):\n    print(\"Loading model!\")\n    model = NarrativeGraph.load(model_name)\nelse:\n    print(\"Creating and saving model!\")\n    model = NarrativeGraph().fit(docs)\n    model.save_to_file(model_name)\n\nmodel.serve_visualizer()\n</code></pre></p>"},{"location":"notebooks/categories/","title":"Categories metadata","text":"In\u00a0[1]: Copied! <pre>single_level_single_label = [\"Politics\", \"Sports\", \"Food\"]\n</pre> single_level_single_label = [\"Politics\", \"Sports\", \"Food\"] In\u00a0[2]: Copied! <pre>single_level_multi_label = [\n    [\"Politics\", \"Celebrities\"],\n    [\"Sports\", \"Celebrities\"],\n    [\"Sports\", \"Food\"]\n]\n</pre> single_level_multi_label = [     [\"Politics\", \"Celebrities\"],     [\"Sports\", \"Celebrities\"],     [\"Sports\", \"Food\"] ] In\u00a0[3]: Copied! <pre>multi_level_multi_label = {\n    \"section\": [\n        [\"Politics\", \"Celebrities\"],\n        [\"Sports\", \"Celebrities\"],\n        [\"Sports\", \"Food\"]\n    ],\n    \"sentiment\": [\"positive\", \"negative\", \"positive\"]\n}\n\nmulti_level_multi_label = [\n    {\"section\": [\"Politics\", \"Celebrities\"], \"sentiment\": \"positive\"},\n    {\"section\": [\"Sports\", \"Celebrities\"], \"sentiment\": \"negative\"},\n    {\"section\": [\"Sports\", \"Food\"], \"sentiment\": \"positive\"}\n]\n</pre> multi_level_multi_label = {     \"section\": [         [\"Politics\", \"Celebrities\"],         [\"Sports\", \"Celebrities\"],         [\"Sports\", \"Food\"]     ],     \"sentiment\": [\"positive\", \"negative\", \"positive\"] }  multi_level_multi_label = [     {\"section\": [\"Politics\", \"Celebrities\"], \"sentiment\": \"positive\"},     {\"section\": [\"Sports\", \"Celebrities\"], \"sentiment\": \"negative\"},     {\"section\": [\"Sports\", \"Food\"], \"sentiment\": \"positive\"} ]"},{"location":"notebooks/categories/#categories-metadata","title":"Categories metadata\u00b6","text":"<p>When you fit a <code>NarrativeGraph</code> object, the <code>categories</code> argument can be given in many different ways. This is because the system supports various levels of categorization.</p>"},{"location":"notebooks/categories/#single-level-single-labels","title":"Single level, single labels\u00b6","text":"<p>The simplest case is when you have on type of category. That could be the newspaper section that a document is from. This is given as a single list with a single label per category.</p>"},{"location":"notebooks/categories/#single-level-multiple-labels","title":"Single level, multiple labels\u00b6","text":"<p>A similar case is when you have one type of category, but where a single document can have multiple labels for that category. It could be newspaper thematic/topical tags. This is given as a single list with a list of labels for each document.</p>"},{"location":"notebooks/categories/#multiple-levels-variable-levels","title":"Multiple levels, variable levels\u00b6","text":"<p>The more complex case is when you have multiple categories, and each of these categories may behave differently. It could be the tags from above in one category (multi-label) and sentiment in another (single-label).</p> <p>This can be given in two ways: a dict with list values or a list with dict entries.</p>"},{"location":"notebooks/demo/","title":"Demo: Creating and inspecting a narrative graph","text":"<p>This notebook will serve as a demo and small tour of some of the core functionalities of a <code>NarrativeGraph</code> object.</p> In\u00a0[1]: Copied! <pre>import time\n\nfrom kagglehub import KaggleDatasetAdapter\nimport kagglehub\n\ndata = kagglehub.dataset_load(\n    KaggleDatasetAdapter.PANDAS,\n    \"rmisra/news-category-dataset\",\n    \"News_Category_Dataset_v3.json\",\n    pandas_kwargs=dict(lines=True),\n)\ndata.head()\n</pre> import time  from kagglehub import KaggleDatasetAdapter import kagglehub  data = kagglehub.dataset_load(     KaggleDatasetAdapter.PANDAS,     \"rmisra/news-category-dataset\",     \"News_Category_Dataset_v3.json\",     pandas_kwargs=dict(lines=True), ) data.head() Out[1]: link headline category short_description authors date 0 https://www.huffpost.com/entry/covid-boosters-... Over 4 Million Americans Roll Up Sleeves For O... U.S. NEWS Health experts said it is too early to predict... Carla K. Johnson, AP 2022-09-23 1 https://www.huffpost.com/entry/american-airlin... American Airlines Flyer Charged, Banned For Li... U.S. NEWS He was subdued by passengers and crew when he ... Mary Papenfuss 2022-09-23 2 https://www.huffpost.com/entry/funniest-tweets... 23 Of The Funniest Tweets About Cats And Dogs ... COMEDY \"Until you have a dog you don't understand wha... Elyse Wanshel 2022-09-23 3 https://www.huffpost.com/entry/funniest-parent... The Funniest Tweets From Parents This Week (Se... PARENTING \"Accidentally put grown-up toothpaste on my to... Caroline Bologna 2022-09-23 4 https://www.huffpost.com/entry/amy-cooper-lose... Woman Who Called Cops On Black Bird-Watcher Lo... U.S. NEWS Amy Cooper accused investment firm Franklin Te... Nina Golgowski 2022-09-22 <p>The columns that we will be using as input for our narrative graph.</p> <ul> <li>Documents: headline + short_description</li> <li>IDs: link, but without the part that is in all of them</li> <li>Timestamps: date</li> <li>Categories: category</li> </ul> <p>There are many categories. We will create a subset with just two of them: U.S. News and Politics.</p> In\u00a0[2]: Copied! <pre># create a sample\nsample = data[data[\"category\"].isin([\"U.S. NEWS\", \"POLITICS\"])].sample(\n    5000, random_state=42\n)\ndocs = sample[\"headline\"] + \"\\n\\n\" + sample[\"short_description\"]\nids = sample[\"link\"].replace(\"https://www.huffpost.com/entry/\", \"\")  # get rit of the first part of the URL\ncategories = sample[\"category\"]\ntimestamps = sample[\"date\"]\n</pre> # create a sample sample = data[data[\"category\"].isin([\"U.S. NEWS\", \"POLITICS\"])].sample(     5000, random_state=42 ) docs = sample[\"headline\"] + \"\\n\\n\" + sample[\"short_description\"] ids = sample[\"link\"].replace(\"https://www.huffpost.com/entry/\", \"\")  # get rit of the first part of the URL categories = sample[\"category\"] timestamps = sample[\"date\"] In\u00a0[3]: Copied! <pre>from narrativegraphs import NarrativeGraph\n\nmodel = NarrativeGraph()\nmodel.fit(docs, doc_ids=ids, categories=categories, timestamps=timestamps)\n</pre> from narrativegraphs import NarrativeGraph  model = NarrativeGraph() model.fit(docs, doc_ids=ids, categories=categories, timestamps=timestamps) <pre>INFO:narrativegraphs.pipeline:Adding 5000 documents to database\n</pre> <pre>INFO:narrativegraphs.pipeline:Extracting triplets\n</pre> <pre>INFO:narrativegraphs.pipeline:Resolving entities and predicates\n</pre> <pre>INFO:narrativegraphs.pipeline:Mapping triplets and tuplets\n</pre> <pre>INFO:narrativegraphs.pipeline:Calculating stats\n</pre> Out[3]: <pre>&lt;narrativegraphs.graphs.NarrativeGraph at 0x177aba7b0&gt;</pre> In\u00a0[4]: Copied! <pre># create server to be viewed in own browser which blocks execution of other cells\n## model.serve_visualizer()\n\n## Or run in the background\nserver = model.serve_visualizer(block=False)\n</pre> # create server to be viewed in own browser which blocks execution of other cells ## model.serve_visualizer()  ## Or run in the background server = model.serve_visualizer(block=False) <pre>INFO:root:Server started in background on port 8001\n</pre> In\u00a0[5]: Copied! <pre>server.stop()\n</pre> server.stop() <pre>INFO:root:Background server stopped\n</pre> <p>Stop it by hitting the stop button on the cell in Jupyter Notebook or hit CTRL+C elsewhere.</p> In\u00a0[6]: Copied! <pre>relation_graph = model.relation_graph_\n</pre> relation_graph = model.relation_graph_ In\u00a0[7]: Copied! <pre>print(type(relation_graph))\n</pre> print(type(relation_graph)) <pre>&lt;class 'networkx.classes.digraph.DiGraph'&gt;\n</pre> In\u00a0[8]: Copied! <pre>print(*list(relation_graph.nodes(data=True))[:3], sep=\"\\n\")\n</pre> print(*list(relation_graph.nodes(data=True))[:3], sep=\"\\n\") <pre>(1, {'id': 1, 'label': 'An App', 'frequency': 1, 'focus': False})\n(2, {'id': 2, 'label': 'Deportation Agents', 'frequency': 1, 'focus': False})\n(3, {'id': 3, 'label': 'that its estimate', 'frequency': 1, 'focus': False})\n</pre> <p>Similarly, entities and relations and everything else can be accessed as <code>pandas.DataFrame</code>s through properties.</p> In\u00a0[9]: Copied! <pre>model.entities_\n</pre> model.entities_ Out[9]: id label frequency doc_frequency spread adjusted_tf_idf first_occurrence last_occurrence alt_labels category 0 1 An App 1 1 0.0002 0.000000 2022-03-11 2022-03-11 [] [POLITICS] 1 2 Deportation Agents 1 1 0.0002 0.000000 2022-03-11 2022-03-11 [] [POLITICS] 2 3 that its estimate 1 1 0.0002 0.000000 2017-12-11 2017-12-11 [] [POLITICS] 3 4 the non-partisan Congressional Budget Office (CBO 1 1 0.0002 0.000000 2017-12-11 2017-12-11 [] [POLITICS] 4 5 The city council 2 2 0.0004 1666.666667 2016-03-29 2016-08-20 [] [POLITICS, POLITICS] ... ... ... ... ... ... ... ... ... ... ... 3416 3417 His Mind 1 1 0.0002 0.000000 2016-04-13 2016-04-13 [] [POLITICS] 3417 3418 The DNC Contenders 1 1 0.0002 0.000000 2017-01-19 2017-01-19 [] [POLITICS] 3418 3419 Interested 1 1 0.0002 0.000000 2017-01-19 2017-01-19 [] [POLITICS] 3419 3420 the stage 1 1 0.0002 0.000000 2017-12-16 2017-12-16 [] [POLITICS] 3420 3421 House Science Committee 1 1 0.0002 0.000000 2014-05-22 2014-05-22 [] [POLITICS] <p>3421 rows \u00d7 10 columns</p> <p>The properties (with trailing <code>_</code>) are nice in that they give back the data in well-known formats that one can continue working with, e.g. NetworkX graphs for graph algorithms and DataFrames for statistical analyses.</p> In\u00a0[10]: Copied! <pre>democrats_matches = model.entities.search(\"democrats\")\ndemocrats_matches[:10]\n</pre> democrats_matches = model.entities.search(\"democrats\") democrats_matches[:10] Out[10]: <pre>[EntityLabel(id=114, label='Democrats'),\n EntityLabel(id=1476, label=\"Democrats' big reform bill\")]</pre> In\u00a0[11]: Copied! <pre>democrats_id = democrats_matches[0].id\n</pre> democrats_id = democrats_matches[0].id <p>And you can create a subgraph that expands from a set of focus nodes and only includes those that pass a filter.</p> In\u00a0[12]: Copied! <pre>from datetime import date\nfrom narrativegraphs import GraphFilter\n\ndemocrats_graph = model.graph.expand_from_focus_entities(\n    [democrats_id],\n    \"relation\",\n    graph_filter=GraphFilter(\n        categories={'category': [\"POLITICS\"]},\n        earliest_date=date(2014, 1, 1)\n    )\n)\n\n# stripping labels to remove some whitespaces\nprint(\"NODES\")\nfor node in democrats_graph.nodes:\n    print(node.id, node.label.strip())\n\nprint(\"\\nEDGES\")\nfor edge in democrats_graph.edges:\n    print(edge.subject_label.strip(), '--', edge.label, '-&gt;', edge.object_label.strip())\n</pre> from datetime import date from narrativegraphs import GraphFilter  democrats_graph = model.graph.expand_from_focus_entities(     [democrats_id],     \"relation\",     graph_filter=GraphFilter(         categories={'category': [\"POLITICS\"]},         earliest_date=date(2014, 1, 1)     ) )  # stripping labels to remove some whitespaces print(\"NODES\") for node in democrats_graph.nodes:     print(node.id, node.label.strip())  print(\"\\nEDGES\") for edge in democrats_graph.edges:     print(edge.subject_label.strip(), '--', edge.label, '-&gt;', edge.object_label.strip()) <pre>NODES\n13 Trump\n23 GOP\n24 Bill\n33 Betsy DeVos\n64 Obama\n77 State\n86 Biden\n113 A majority\n114 Democrats\n143 this week's \"Candidate Confessional\n183 Jefferson Jackson Dinner\n260 Chuck Schumer\n325 A Landslide\n347 A Run\n411 Liberals\n428 health care\n476 different findings\n537 Record Donations\n603 an even more ambitious vision\n606 Planned Parenthood Shooting\n616 A candidate\n638 Republican Mike DeWine\n678 Tehran\n682 Anthony Weiner\n701 judicial nominee Steven Menashi\n739 This Billionaire Environmental Activist\n908 Pennsylvania Republican\n953 Republican Lt. Gov. Kim Guadagno\n1067 the ballot box\n1068 Special Election\n1069 Resources\n1219 more clarity\n1320 world leaders\n1322 Eyeing 2018 Senate Takeover\n1366 key states\n1369 Little Time\n1440 Hugh Hewitt\n1572 The Longest-Serving Woman\n1748 Hemp\n1778 \u2018Hostage Czar\n1858 Nationwide Day\n1933 Black candidates\n1941 the Call\n2026 For Independent Commission\n2073 Republican Cory Gardner\n2174 Rep. Ruben Kihuen\n2219 Renewed Push\n2271 Oversight Committee\n2387 California Assembly Member Roger Hernandez\n2423 Their Nadir\n2440 To End 'Corporate Culture\n2454 Gubernatorial Primary\n2490 A New Committee\n2629 Probe\n2665 About Her\n2862 An Investigation\n2867 over an\n2988 Rep. Jamie Raskin\n3007 Short But Outperform Expectations\n3126 Cuomo\n3395 their worst electoral position\n\nEDGES\nTrump -- puts -&gt; the Call\nTrump -- is -&gt; health care\nTrump -- is -&gt; A candidate\nGOP -- challenging, Stick -&gt; Trump\nGOP -- Considering, Wanted -&gt; Obama\nGOP -- Moves -&gt; Democrats\nBill -- Survives -&gt; GOP\nBill -- strip -&gt; health care\nBill -- Points -&gt; Trump\nObama -- ripped -&gt; Trump\nState -- Explains -&gt; Trump\nBiden -- slammed -&gt; Trump\nDemocrats -- concede -&gt; A majority\nDemocrats -- discuss -&gt; this week's \"Candidate Confessional\nDemocrats -- Rename -&gt; Jefferson Jackson Dinner\nDemocrats -- Turns -&gt; A Run\nDemocrats -- Boycotting, participated, needs -&gt; Trump\nDemocrats -- advanced, calls -&gt; Bill\nDemocrats -- Emphasizes -&gt; health care\nDemocrats -- Flip, Wins -&gt; State\nDemocrats -- Having -&gt; different findings\nDemocrats -- Rake -&gt; Record Donations\nDemocrats -- Lay -&gt; an even more ambitious vision\nDemocrats -- React -&gt; Planned Parenthood Shooting\nDemocrats -- Faces -&gt; Republican Mike DeWine\nDemocrats -- Booted -&gt; Anthony Weiner\nDemocrats -- Asks -&gt; judicial nominee Steven Menashi\nDemocrats -- fled, Push -&gt; Obama\nDemocrats -- Defeated -&gt; Republican Lt. Gov. Kim Guadagno\nDemocrats -- Wins -&gt; Special Election\nDemocrats -- dominated -&gt; the ballot box\nDemocrats -- Pumped -&gt; Resources\nDemocrats -- Demands -&gt; more clarity\nDemocrats -- assured -&gt; world leaders\nDemocrats -- Begins -&gt; Eyeing 2018 Senate Takeover\nDemocrats -- lead -&gt; key states\nDemocrats -- Having -&gt; Little Time\nDemocrats -- Became -&gt; The Longest-Serving Woman\nDemocrats -- agree -&gt; Hemp\nDemocrats -- calls -&gt; \u2018Hostage Czar\nDemocrats -- lead -&gt; Nationwide Day\nDemocrats -- nominate -&gt; Black candidates\nDemocrats -- Renew -&gt; the Call\nDemocrats -- Push -&gt; For Independent Commission\nDemocrats -- lose -&gt; Republican Cory Gardner\nDemocrats -- calls -&gt; Rep. Ruben Kihuen\nDemocrats -- Make -&gt; Renewed Push\nDemocrats -- Asks -&gt; Oversight Committee\nDemocrats -- needs -&gt; A Landslide\nDemocrats -- Are -&gt; Their Nadir\nDemocrats -- Push -&gt; To End 'Corporate Culture\nDemocrats -- Compete -&gt; Gubernatorial Primary\nDemocrats -- Wanted -&gt; A New Committee\nDemocrats -- gave -&gt; Betsy DeVos\nDemocrats -- calls -&gt; Probe\nDemocrats -- is -&gt; About Her\nDemocrats -- Wanted -&gt; An Investigation\nDemocrats -- presided -&gt; over an\nDemocrats -- Tap -&gt; Rep. Jamie Raskin\nDemocrats -- Coming -&gt; Short But Outperform Expectations\nDemocrats -- calls -&gt; Cuomo\nDemocrats -- Welcome, Supporting -&gt; Biden\nDemocrats -- Are -&gt; their worst electoral position\nChuck Schumer -- Warns, Be -&gt; Democrats\nChuck Schumer -- Vows -&gt; Trump\nLiberals -- puts -&gt; Democrats\nA candidate -- is -&gt; Democrats\nTehran -- seeing -&gt; Democrats\nThis Billionaire Environmental Activist -- Picks -&gt; Democrats\nPennsylvania Republican -- Wanted -&gt; Democrats\nHugh Hewitt -- backed -&gt; Democrats\nCalifornia Assembly Member Roger Hernandez -- challenging -&gt; Democrats\n</pre> <p>We can save the model for later use, especially if we have a lot of documents that takes a while to process.</p> In\u00a0[13]: Copied! <pre>model.save_to_file(\"demo.db\", overwrite=True)\n</pre> model.save_to_file(\"demo.db\", overwrite=True) <p>And we can load it from that saved file.</p> In\u00a0[14]: Copied! <pre>model = NarrativeGraph.load(\"demo\")\n</pre> model = NarrativeGraph.load(\"demo\")"},{"location":"notebooks/demo/#demo-creating-and-inspecting-a-narrative-graph","title":"Demo: Creating and inspecting a narrative graph\u00b6","text":""},{"location":"notebooks/demo/#data-setup","title":"Data setup\u00b6","text":"<p>For this demo notebook, we will be using News Category Dataset [1, 2] available on Kagglehub because it has short texts, timestamps and are categorized.</p>"},{"location":"notebooks/demo/#creating-the-model","title":"Creating the model\u00b6","text":"<p>Once we have our list of documents, which is the only required input, and extra metadata in aligned lists, we can create a narrative graph.</p>"},{"location":"notebooks/demo/#inspecting-the-model-visually","title":"Inspecting the model visually\u00b6","text":"<p>One of the key features of the narrativegraphs package is that it lets a user inspect the output interactively in a browser-based visualizer. It is hosted directly on your machine by the Python package \u2013 no extra dependencies required. This is achieved with the one line below.</p> <p>Click the link in the log messages to open in your browser.</p>"},{"location":"notebooks/demo/#inspecting-and-accessing-the-model-programmatically","title":"Inspecting and accessing the model programmatically\u00b6","text":"<p>The graph consists of entities as nodes and their relations or cooccurrences as edges. These, along with the data that back them, like documents and extracted semantic triplets, can be retrieved from the model through properties or service attributes.</p>"},{"location":"notebooks/demo/#attributes","title":"Attributes\u00b6","text":"<p>We can get the graph as a whole, as NetworkX graph, through the properties <code>.relation_graph_</code> and <code>.cooccurrence_graph_</code>.</p>"},{"location":"notebooks/demo/#service-attributes","title":"Service attributes\u00b6","text":"<p>However, the service attributes offer more control and may be especially handy if the model is quite big, so that you do not necessarily want everything spit out at once.</p> <p>For instance, you can search for entities with the <code>entities</code> service.</p>"},{"location":"notebooks/demo/#saving-and-loading-the-model","title":"Saving and loading the model\u00b6","text":""},{"location":"notebooks/demo/#references","title":"References\u00b6","text":"<p>[1] Misra, Rishabh. \"News Category Dataset.\" arXiv preprint arXiv:2209.11429 (2022).</p> <p>[2] Misra, Rishabh and Jigyasa Grover. \"Sculpting Data for ML: The first act of Machine Learning.\" ISBN 9798585463570 (2021).</p>"}]}